{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.append( \"../code/\" )\n",
    "\n",
    "from relationnet.nn import ImageRelationNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import the relation net code. Its not very generalized at the moment so you need to get your dataframe in a particular format for it to work. I add it to the PYTHONPATH here so I can import it easily, but you could move in to the current directory and get it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = Path( \"/home/lewis/Work/Employment/fellowshipai/fashion/data/streetstyle/\" )\n",
    "model_dir = Path( \"/home/lewis/Work/Employment/fellowshipai/fashion/submission\" )\n",
    "im_dir = data_root_dir/\"streetstyle27k_cropped\"\n",
    "csv = data_root_dir/\"ss27k_labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna extract specific classes from the csv and create a new dataframe from it, keeping only those classes. Have a read below for why we're only using one class and not all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8976</th>\n",
       "      <td>1a23c7470bfccfd65c068ea12299b5a8_7479875228365...</td>\n",
       "      <td>clothing_category_t-shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13467</th>\n",
       "      <td>1e80fcaa80f96a744233886cacfe97d8_7978015252306...</td>\n",
       "      <td>clothing_category_dress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>7d39a00c898b09501b49098fe7034fe6_9244181586496...</td>\n",
       "      <td>clothing_category_shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10605</th>\n",
       "      <td>863b16ef17ba5b4b9be4f9c8cb17f237_8505265244947...</td>\n",
       "      <td>clothing_category_sweater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4254</th>\n",
       "      <td>8ea7166097ac1e5d3a89f08aab7b4988_6437852106281...</td>\n",
       "      <td>clothing_category_t-shirt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   image  \\\n",
       "8976   1a23c7470bfccfd65c068ea12299b5a8_7479875228365...   \n",
       "13467  1e80fcaa80f96a744233886cacfe97d8_7978015252306...   \n",
       "6069   7d39a00c898b09501b49098fe7034fe6_9244181586496...   \n",
       "10605  863b16ef17ba5b4b9be4f9c8cb17f237_8505265244947...   \n",
       "4254   8ea7166097ac1e5d3a89f08aab7b4988_6437852106281...   \n",
       "\n",
       "                           label  \n",
       "8976   clothing_category_t-shirt  \n",
       "13467    clothing_category_dress  \n",
       "6069     clothing_category_shirt  \n",
       "10605  clothing_category_sweater  \n",
       "4254   clothing_category_t-shirt  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = set( [ \"clothing_category_dress\", \"clothing_category_outwear\", \"clothing_category_shirt\", \n",
    "              \"clothing_category_suit\", \"clothing_category_sweater\", \"clothing_category_tank_top\",\n",
    "             \"clothing_category_t-shirt\" ] )\n",
    "\n",
    "ims = { \"image\": [], \"label\": [] }\n",
    "with open( csv, \"r\" ) as f:\n",
    "    for line in f:\n",
    "        im, cls = line.split( \",\" )\n",
    "        cls = set( cls.split( \" \" ) )\n",
    "        \n",
    "        inter = targets.intersection( cls )\n",
    "        if len( inter ) == 1:\n",
    "            ims[\"image\"].append( im )\n",
    "            ims[\"label\"].append( list( inter )[0] )\n",
    "        elif len( inter ) > 1:\n",
    "            print( im, cls )\n",
    "            \n",
    "df = pd.DataFrame.from_dict( ims ).sample( frac=1 )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would train on the entire dataset but there is an issue in that with how relation nets are set up. They train based on a single relation score between two images. For a single label problem this is easy, images are related if they are of the same class and are unrelated otherwise. Its more complicated in a multi-label scenario. If the labels are not exclusive, i.e. one does not preclude the other, you get the issue of false negatives/positives. \n",
    "\n",
    "For example: if you are training clothing category and colour on the same net (same weights) and you have two examples, (dress,red) and (jumper,red). When training using clothing category as the relation metric, you score them as not similar and the net updates its weights. When you then score the same set on colour you now score them as similar. To the net, it doesn't know the difference between colour and clothing type - it does not distinguish between different types/classes of similarity, and so you introduce noise to the net in the form of either a false positive of dress being similar to jumper, or a false negative of red not being similar to red.\n",
    "\n",
    "Even if you don't use the same images for the comparison, you are still comparing on the same latent features (the cnn representation for 'red' etc). There are ways around this problem, e.g. having a separate head in the relation net for each exclusive class (clothing category is exclusive in that you can't be labelled wearing both a dress and a t-shirt, and so we only need one head for the entire category) and going further a paper (https://arxiv.org/pdf/1805.12501.pdf) introduces the idea of linking these heads with a shared loss function so they can share semantic information (e.g. gender=woman predicts clothing_category=dress). We may try these extensions, depending on how large our dataset is (which determines whether we are constrained to few-shot approaches or not) however that is not the direction we are going right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15890, 15890)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df[\"image\"].unique() ), len( df[\"image\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relational nets are meant to be scored on unseen _classes_, not just unseen images. This doesn't really work for us since we only have 7 classes and on each episode we want to train with 5 of them, can't hold another 5 out for the validation set. This is fine though, we aren't leaking data. The main justification behind this (I think) is to promote the few-shot capabilities of the architecture in working with totally unseen classes. Fundamentally the architecture is not trying to learn how to compare classes but to compare abstract images - to find comonalities. \n",
    "\n",
    "Since the default implementation is set up to split the validation set like this, we're gonna create our own manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>d747f6321f54aa8b674d08e6a1381f59_7203359563331...</td>\n",
       "      <td>clothing_category_t-shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6443</th>\n",
       "      <td>0b6c3cad49cb7b06d68c337de11b8ad9_7532315915052...</td>\n",
       "      <td>clothing_category_t-shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>73fa0b081f9c82ab2e0ff80220e3b13f_7022395166408...</td>\n",
       "      <td>clothing_category_shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8523</th>\n",
       "      <td>921d82a93037bf75d3abd83606dbaab0_8507080194065...</td>\n",
       "      <td>clothing_category_t-shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>824a98c7ca6c3dd9fddce42819618de7_9251100104920...</td>\n",
       "      <td>clothing_category_sweater</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  image  \\\n",
       "2377  d747f6321f54aa8b674d08e6a1381f59_7203359563331...   \n",
       "6443  0b6c3cad49cb7b06d68c337de11b8ad9_7532315915052...   \n",
       "1423  73fa0b081f9c82ab2e0ff80220e3b13f_7022395166408...   \n",
       "8523  921d82a93037bf75d3abd83606dbaab0_8507080194065...   \n",
       "1144  824a98c7ca6c3dd9fddce42819618de7_9251100104920...   \n",
       "\n",
       "                          label  \n",
       "2377  clothing_category_t-shirt  \n",
       "6443  clothing_category_t-shirt  \n",
       "1423    clothing_category_shirt  \n",
       "8523  clothing_category_t-shirt  \n",
       "1144  clothing_category_sweater  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pcnt = 0.2\n",
    "val_len = int( len( df ) * val_pcnt )\n",
    "\n",
    "val_df, train_df = df[:val_len], df[val_len:]\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a set of transformations to apply to each image as it is retrieved. These can be any torch transformation, have a look at the docs for more. The original paper used fixed rotations (0,90,180,270) applied to an entire episode of images. We do it a bit differently here by applying a random rotation per image instead of per episode. It does reduce the accuracy on the benchmark set, as to be expected, but not by much and I think random rotations lead to a more robust model anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = transforms.Normalize( mean=[0.92206], std=[0.08426] ) # we take these straight from the paper although they may not be appropriate for our dataset. It shouldn't affect accuracy too much however.\n",
    "rot = transforms.RandomRotation( [0, 364] )\n",
    "tsf = transforms.Compose( [rot, transforms.ToTensor(), norm] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: size needs to be 84 because of the nn dimensions, need to work out a way to infer the proper dimensions/view from size\n",
    "rn = ImageRelationNetwork( df=train_df, val_df=val_df, data_dir=im_dir, model_dir=model_dir, data_num_dims=3, padding=0, shuffle=True, tsf=tsf, size=84 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning train\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.20073333333333332\n",
      "Saving model\n",
      "Episode: 100 Loss: 0.1599491983652115\n",
      "Episode: 200 Loss: 0.1574077606201172\n",
      "Episode: 300 Loss: 0.15948539972305298\n",
      "Episode: 400 Loss: 0.1593618094921112\n",
      "Episode: 500 Loss: 0.15747348964214325\n",
      "Episode: 600 Loss: 0.1575508862733841\n",
      "Episode: 700 Loss: 0.16456301510334015\n",
      "Episode: 800 Loss: 0.15151704847812653\n",
      "Episode: 900 Loss: 0.1609458327293396\n",
      "Episode: 1000 Loss: 0.16191701591014862\n",
      "Episode: 1100 Loss: 0.15128806233406067\n",
      "Episode: 1200 Loss: 0.15386083722114563\n",
      "Episode: 1300 Loss: 0.16425058245658875\n",
      "Episode: 1400 Loss: 0.15359683334827423\n",
      "Episode: 1500 Loss: 0.1600746512413025\n",
      "Episode: 1600 Loss: 0.15862910449504852\n",
      "Episode: 1700 Loss: 0.14875391125679016\n",
      "Episode: 1800 Loss: 0.15047527849674225\n",
      "Episode: 1900 Loss: 0.14869792759418488\n",
      "Episode: 2000 Loss: 0.16144363582134247\n",
      "Episode: 2100 Loss: 0.1498306542634964\n",
      "Episode: 2200 Loss: 0.15337921679019928\n",
      "Episode: 2300 Loss: 0.14599832892417908\n",
      "Episode: 2400 Loss: 0.1529267430305481\n",
      "Episode: 2500 Loss: 0.16244488954544067\n",
      "Episode: 2600 Loss: 0.14546798169612885\n",
      "Episode: 2700 Loss: 0.14508049190044403\n",
      "Episode: 2800 Loss: 0.13811293244361877\n",
      "Episode: 2900 Loss: 0.13987138867378235\n",
      "Episode: 3000 Loss: 0.14272527396678925\n",
      "Episode: 3100 Loss: 0.16612602770328522\n",
      "Episode: 3200 Loss: 0.1443190723657608\n",
      "Episode: 3300 Loss: 0.14623643457889557\n",
      "Episode: 3400 Loss: 0.14002543687820435\n",
      "Episode: 3500 Loss: 0.14748433232307434\n",
      "Episode: 3600 Loss: 0.1626044511795044\n",
      "Episode: 3700 Loss: 0.1623830646276474\n",
      "Episode: 3800 Loss: 0.14011971652507782\n",
      "Episode: 3900 Loss: 0.14570218324661255\n",
      "Episode: 4000 Loss: 0.16688910126686096\n",
      "Episode: 4100 Loss: 0.14090503752231598\n",
      "Episode: 4200 Loss: 0.14403757452964783\n",
      "Episode: 4300 Loss: 0.13923992216587067\n",
      "Episode: 4400 Loss: 0.14713577926158905\n",
      "Episode: 4500 Loss: 0.136529803276062\n",
      "Episode: 4600 Loss: 0.1413293480873108\n",
      "Episode: 4700 Loss: 0.1485297679901123\n",
      "Episode: 4800 Loss: 0.13486579060554504\n",
      "Episode: 4900 Loss: 0.14750592410564423\n",
      "Episode: 5000 Loss: 0.16067811846733093\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.3402133333333333\n",
      "Saving model\n",
      "Episode: 5100 Loss: 0.14398744702339172\n",
      "Episode: 5200 Loss: 0.14586487412452698\n",
      "Episode: 5300 Loss: 0.1333446204662323\n",
      "Episode: 5400 Loss: 0.14589129388332367\n",
      "Episode: 5500 Loss: 0.14576570689678192\n",
      "Episode: 5600 Loss: 0.13933776319026947\n",
      "Episode: 5700 Loss: 0.1394203156232834\n",
      "Episode: 5800 Loss: 0.1405262053012848\n",
      "Episode: 5900 Loss: 0.13672393560409546\n",
      "Episode: 6000 Loss: 0.15322749316692352\n",
      "Episode: 6100 Loss: 0.1433495283126831\n",
      "Episode: 6200 Loss: 0.14289838075637817\n",
      "Episode: 6300 Loss: 0.14428068697452545\n",
      "Episode: 6400 Loss: 0.14295397698879242\n",
      "Episode: 6500 Loss: 0.13646921515464783\n",
      "Episode: 6600 Loss: 0.1381016969680786\n",
      "Episode: 6700 Loss: 0.13456638157367706\n",
      "Episode: 6800 Loss: 0.13585339486598969\n",
      "Episode: 6900 Loss: 0.13826365768909454\n",
      "Episode: 7000 Loss: 0.13979694247245789\n",
      "Episode: 7100 Loss: 0.13118425011634827\n",
      "Episode: 7200 Loss: 0.15966752171516418\n",
      "Episode: 7300 Loss: 0.1368904858827591\n",
      "Episode: 7400 Loss: 0.14483752846717834\n",
      "Episode: 7500 Loss: 0.139276921749115\n",
      "Episode: 7600 Loss: 0.13409478962421417\n",
      "Episode: 7700 Loss: 0.12954390048980713\n",
      "Episode: 7800 Loss: 0.1592346578836441\n",
      "Episode: 7900 Loss: 0.13512733578681946\n",
      "Episode: 8000 Loss: 0.1416064202785492\n",
      "Episode: 8100 Loss: 0.13275259733200073\n",
      "Episode: 8200 Loss: 0.13628536462783813\n",
      "Episode: 8300 Loss: 0.13383828103542328\n",
      "Episode: 8400 Loss: 0.15587076544761658\n",
      "Episode: 8500 Loss: 0.1268063485622406\n",
      "Episode: 8600 Loss: 0.1574886292219162\n",
      "Episode: 8700 Loss: 0.13064049184322357\n",
      "Episode: 8800 Loss: 0.12556689977645874\n",
      "Episode: 8900 Loss: 0.12975996732711792\n",
      "Episode: 9000 Loss: 0.14785918593406677\n",
      "Episode: 9100 Loss: 0.12632565200328827\n",
      "Episode: 9200 Loss: 0.12532612681388855\n",
      "Episode: 9300 Loss: 0.13955433666706085\n",
      "Episode: 9400 Loss: 0.13233676552772522\n",
      "Episode: 9500 Loss: 0.1329943686723709\n",
      "Episode: 9600 Loss: 0.13801658153533936\n",
      "Episode: 9700 Loss: 0.13369368016719818\n",
      "Episode: 9800 Loss: 0.15234507620334625\n",
      "Episode: 9900 Loss: 0.13106127083301544\n",
      "Episode: 10000 Loss: 0.125569149851799\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.3692\n",
      "Saving model\n",
      "Episode: 10100 Loss: 0.13261154294013977\n",
      "Episode: 10200 Loss: 0.16331377625465393\n",
      "Episode: 10300 Loss: 0.15652936697006226\n",
      "Episode: 10400 Loss: 0.12363132834434509\n",
      "Episode: 10500 Loss: 0.13019131124019623\n",
      "Episode: 10600 Loss: 0.1312451809644699\n",
      "Episode: 10700 Loss: 0.14134430885314941\n",
      "Episode: 10800 Loss: 0.13924576342105865\n",
      "Episode: 10900 Loss: 0.13136136531829834\n",
      "Episode: 11000 Loss: 0.1241740733385086\n",
      "Episode: 11100 Loss: 0.14350526034832\n",
      "Episode: 11200 Loss: 0.11709340661764145\n",
      "Episode: 11300 Loss: 0.12108135223388672\n",
      "Episode: 11400 Loss: 0.15415100753307343\n",
      "Episode: 11500 Loss: 0.13240402936935425\n",
      "Episode: 11600 Loss: 0.13092996180057526\n",
      "Episode: 11700 Loss: 0.13162373006343842\n",
      "Episode: 11800 Loss: 0.14096273481845856\n",
      "Episode: 11900 Loss: 0.12576474249362946\n",
      "Episode: 12000 Loss: 0.13218295574188232\n",
      "Episode: 12100 Loss: 0.11398700624704361\n",
      "Episode: 12200 Loss: 0.12700214982032776\n",
      "Episode: 12300 Loss: 0.1336306631565094\n",
      "Episode: 12400 Loss: 0.1333041936159134\n",
      "Episode: 12500 Loss: 0.1254928708076477\n",
      "Episode: 12600 Loss: 0.12933875620365143\n",
      "Episode: 12700 Loss: 0.13084207475185394\n",
      "Episode: 12800 Loss: 0.1362289935350418\n",
      "Episode: 12900 Loss: 0.11628041416406631\n",
      "Episode: 13000 Loss: 0.13005690276622772\n",
      "Episode: 13100 Loss: 0.12166377902030945\n",
      "Episode: 13200 Loss: 0.11999285221099854\n",
      "Episode: 13300 Loss: 0.12731467187404633\n",
      "Episode: 13400 Loss: 0.13494791090488434\n",
      "Episode: 13500 Loss: 0.13200390338897705\n",
      "Episode: 13600 Loss: 0.12640157341957092\n",
      "Episode: 13700 Loss: 0.12792694568634033\n",
      "Episode: 13800 Loss: 0.11907221376895905\n",
      "Episode: 13900 Loss: 0.1611270010471344\n",
      "Episode: 14000 Loss: 0.12962910532951355\n",
      "Episode: 14100 Loss: 0.1188119575381279\n",
      "Episode: 14200 Loss: 0.1510993242263794\n",
      "Episode: 14300 Loss: 0.15694403648376465\n",
      "Episode: 14400 Loss: 0.14793086051940918\n",
      "Episode: 14500 Loss: 0.1341351568698883\n",
      "Episode: 14600 Loss: 0.13190247118473053\n",
      "Episode: 14700 Loss: 0.12147904187440872\n",
      "Episode: 14800 Loss: 0.14005467295646667\n",
      "Episode: 14900 Loss: 0.1618146300315857\n",
      "Episode: 15000 Loss: 0.12940581142902374\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.38264000000000004\n",
      "Saving model\n",
      "Episode: 15100 Loss: 0.11931838095188141\n",
      "Episode: 15200 Loss: 0.13022108376026154\n",
      "Episode: 15300 Loss: 0.12030994892120361\n",
      "Episode: 15400 Loss: 0.14483889937400818\n",
      "Episode: 15500 Loss: 0.15158464014530182\n",
      "Episode: 15600 Loss: 0.12061190605163574\n",
      "Episode: 15700 Loss: 0.1297452747821808\n",
      "Episode: 15800 Loss: 0.12337373942136765\n",
      "Episode: 15900 Loss: 0.1304827630519867\n",
      "Episode: 16000 Loss: 0.14193283021450043\n",
      "Episode: 16100 Loss: 0.11881484091281891\n",
      "Episode: 16200 Loss: 0.13822762668132782\n",
      "Episode: 16300 Loss: 0.12648989260196686\n",
      "Episode: 16400 Loss: 0.1487463414669037\n",
      "Episode: 16500 Loss: 0.1307818740606308\n",
      "Episode: 16600 Loss: 0.12731212377548218\n",
      "Episode: 16700 Loss: 0.12084808200597763\n",
      "Episode: 16800 Loss: 0.1236833706498146\n",
      "Episode: 16900 Loss: 0.12205586582422256\n",
      "Episode: 17000 Loss: 0.15423071384429932\n",
      "Episode: 17100 Loss: 0.12505018711090088\n",
      "Episode: 17200 Loss: 0.12042757123708725\n",
      "Episode: 17300 Loss: 0.11958808451890945\n",
      "Episode: 17400 Loss: 0.1165190115571022\n",
      "Episode: 17500 Loss: 0.12145375460386276\n",
      "Episode: 17600 Loss: 0.15824981033802032\n",
      "Episode: 17700 Loss: 0.15058109164237976\n",
      "Episode: 17800 Loss: 0.11930404603481293\n",
      "Episode: 17900 Loss: 0.14027222990989685\n",
      "Episode: 18000 Loss: 0.126214399933815\n",
      "Episode: 18100 Loss: 0.12380042672157288\n",
      "Episode: 18200 Loss: 0.13031499087810516\n",
      "Episode: 18300 Loss: 0.1365661919116974\n",
      "Episode: 18400 Loss: 0.11647475510835648\n",
      "Episode: 18500 Loss: 0.12823830544948578\n",
      "Episode: 18600 Loss: 0.1464950293302536\n",
      "Episode: 18700 Loss: 0.15716984868049622\n",
      "Episode: 18800 Loss: 0.12591829895973206\n",
      "Episode: 18900 Loss: 0.15628308057785034\n",
      "Episode: 19000 Loss: 0.12930302321910858\n",
      "Episode: 19100 Loss: 0.11276943981647491\n",
      "Episode: 19200 Loss: 0.14514337480068207\n",
      "Episode: 19300 Loss: 0.12922371923923492\n",
      "Episode: 19400 Loss: 0.12296696752309799\n",
      "Episode: 19500 Loss: 0.14724396169185638\n",
      "Episode: 19600 Loss: 0.11574789136648178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 19700 Loss: 0.11790784448385239\n",
      "Episode: 19800 Loss: 0.12241291999816895\n",
      "Episode: 19900 Loss: 0.12180857360363007\n",
      "Episode: 20000 Loss: 0.1167541891336441\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.39681333333333335\n",
      "Saving model\n",
      "Episode: 20100 Loss: 0.12754984200000763\n",
      "Episode: 20200 Loss: 0.12351611256599426\n",
      "Episode: 20300 Loss: 0.11808043718338013\n",
      "Episode: 20400 Loss: 0.1522122323513031\n",
      "Episode: 20500 Loss: 0.12074363976716995\n",
      "Episode: 20600 Loss: 0.13170786201953888\n",
      "Episode: 20700 Loss: 0.12210224568843842\n",
      "Episode: 20800 Loss: 0.12174086272716522\n",
      "Episode: 20900 Loss: 0.12744639813899994\n",
      "Episode: 21000 Loss: 0.1485367864370346\n",
      "Episode: 21100 Loss: 0.11839252710342407\n",
      "Episode: 21200 Loss: 0.14569413661956787\n",
      "Episode: 21300 Loss: 0.123101145029068\n",
      "Episode: 21400 Loss: 0.12222810089588165\n",
      "Episode: 21500 Loss: 0.1347251534461975\n",
      "Episode: 21600 Loss: 0.11882774531841278\n",
      "Episode: 21700 Loss: 0.10868573188781738\n",
      "Episode: 21800 Loss: 0.13400529325008392\n",
      "Episode: 21900 Loss: 0.12704241275787354\n",
      "Episode: 22000 Loss: 0.1548938751220703\n",
      "Episode: 22100 Loss: 0.11720738559961319\n",
      "Episode: 22200 Loss: 0.11401571333408356\n",
      "Episode: 22300 Loss: 0.11079087108373642\n",
      "Episode: 22400 Loss: 0.10667917132377625\n",
      "Episode: 22500 Loss: 0.12417048960924149\n",
      "Episode: 22600 Loss: 0.12015779316425323\n",
      "Episode: 22700 Loss: 0.13152027130126953\n",
      "Episode: 22800 Loss: 0.12015922367572784\n",
      "Episode: 22900 Loss: 0.1396089643239975\n",
      "Episode: 23000 Loss: 0.11875326186418533\n",
      "Episode: 23100 Loss: 0.12345309555530548\n",
      "Episode: 23200 Loss: 0.11658335477113724\n",
      "Episode: 23300 Loss: 0.10964704304933548\n",
      "Episode: 23400 Loss: 0.12085732072591782\n",
      "Episode: 23500 Loss: 0.1453176736831665\n",
      "Episode: 23600 Loss: 0.13224999606609344\n",
      "Episode: 23700 Loss: 0.14433850347995758\n",
      "Episode: 23800 Loss: 0.11494752019643784\n",
      "Episode: 23900 Loss: 0.11708004772663116\n",
      "Episode: 24000 Loss: 0.11219488829374313\n",
      "Episode: 24100 Loss: 0.12769544124603271\n",
      "Episode: 24200 Loss: 0.12103567272424698\n",
      "Episode: 24300 Loss: 0.13334816694259644\n",
      "Episode: 24400 Loss: 0.13998493552207947\n",
      "Episode: 24500 Loss: 0.1225205659866333\n",
      "Episode: 24600 Loss: 0.11263101547956467\n",
      "Episode: 24700 Loss: 0.12064032256603241\n",
      "Episode: 24800 Loss: 0.11550561338663101\n",
      "Episode: 24900 Loss: 0.11801358312368393\n",
      "Episode: 25000 Loss: 0.1163967177271843\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.41256\n",
      "Saving model\n",
      "Episode: 25100 Loss: 0.12939274311065674\n",
      "Episode: 25200 Loss: 0.11448846757411957\n",
      "Episode: 25300 Loss: 0.14328311383724213\n",
      "Episode: 25400 Loss: 0.11360093206167221\n",
      "Episode: 25500 Loss: 0.11729275435209274\n",
      "Episode: 25600 Loss: 0.10315706580877304\n",
      "Episode: 25700 Loss: 0.1496346890926361\n",
      "Episode: 25800 Loss: 0.12059043347835541\n",
      "Episode: 25900 Loss: 0.12838584184646606\n",
      "Episode: 26000 Loss: 0.1515146642923355\n",
      "Episode: 26100 Loss: 0.12839125096797943\n",
      "Episode: 26200 Loss: 0.1276101917028427\n",
      "Episode: 26300 Loss: 0.11344552785158157\n",
      "Episode: 26400 Loss: 0.14183193445205688\n",
      "Episode: 26500 Loss: 0.11393904685974121\n",
      "Episode: 26600 Loss: 0.11120165884494781\n",
      "Episode: 26700 Loss: 0.11591418087482452\n",
      "Episode: 26800 Loss: 0.12422996014356613\n",
      "Episode: 26900 Loss: 0.11356841772794724\n",
      "Episode: 27000 Loss: 0.11704913526773453\n",
      "Episode: 27100 Loss: 0.12095867097377777\n",
      "Episode: 27200 Loss: 0.15161564946174622\n",
      "Episode: 27300 Loss: 0.11581594496965408\n",
      "Episode: 27400 Loss: 0.11791068315505981\n",
      "Episode: 27500 Loss: 0.10935591906309128\n",
      "Episode: 27600 Loss: 0.11589863151311874\n",
      "Episode: 27700 Loss: 0.10718043148517609\n",
      "Episode: 27800 Loss: 0.11925214529037476\n",
      "Episode: 27900 Loss: 0.11746619641780853\n",
      "Episode: 28000 Loss: 0.11131845414638519\n",
      "Episode: 28100 Loss: 0.12013784795999527\n",
      "Episode: 28200 Loss: 0.1250396966934204\n",
      "Episode: 28300 Loss: 0.1196693703532219\n",
      "Episode: 28400 Loss: 0.10414893925189972\n",
      "Episode: 28500 Loss: 0.11676489561796188\n",
      "Episode: 28600 Loss: 0.1530580371618271\n",
      "Episode: 28700 Loss: 0.13120321929454803\n",
      "Episode: 28800 Loss: 0.10810428857803345\n",
      "Episode: 28900 Loss: 0.1311272233724594\n",
      "Episode: 29000 Loss: 0.11892440915107727\n",
      "Episode: 29100 Loss: 0.11291352659463882\n",
      "Episode: 29200 Loss: 0.11591368913650513\n",
      "Episode: 29300 Loss: 0.12439047545194626\n",
      "Episode: 29400 Loss: 0.12074948102235794\n",
      "Episode: 29500 Loss: 0.11220698803663254\n",
      "Episode: 29600 Loss: 0.10342730581760406\n",
      "Episode: 29700 Loss: 0.12009366601705551\n",
      "Episode: 29800 Loss: 0.14178821444511414\n",
      "Episode: 29900 Loss: 0.1418454647064209\n",
      "Episode: 30000 Loss: 0.1158386841416359\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.41847999999999996\n",
      "Saving model\n",
      "Episode: 30100 Loss: 0.11789082735776901\n",
      "Episode: 30200 Loss: 0.11384893953800201\n",
      "Episode: 30300 Loss: 0.14831767976284027\n",
      "Episode: 30400 Loss: 0.13690315186977386\n",
      "Episode: 30500 Loss: 0.1369049847126007\n",
      "Episode: 30600 Loss: 0.11244900524616241\n",
      "Episode: 30700 Loss: 0.10744857788085938\n",
      "Episode: 30800 Loss: 0.14006690680980682\n",
      "Episode: 30900 Loss: 0.11588101089000702\n",
      "Episode: 31000 Loss: 0.10613442212343216\n",
      "Episode: 31100 Loss: 0.15337488055229187\n",
      "Episode: 31200 Loss: 0.1276148110628128\n",
      "Episode: 31300 Loss: 0.13178570568561554\n",
      "Episode: 31400 Loss: 0.11344313621520996\n",
      "Episode: 31500 Loss: 0.12326730042695999\n",
      "Episode: 31600 Loss: 0.13345494866371155\n",
      "Episode: 31700 Loss: 0.10616037994623184\n",
      "Episode: 31800 Loss: 0.1094573363661766\n",
      "Episode: 31900 Loss: 0.11661882698535919\n",
      "Episode: 32000 Loss: 0.11673060804605484\n",
      "Episode: 32100 Loss: 0.11250299960374832\n",
      "Episode: 32200 Loss: 0.11491662263870239\n",
      "Episode: 32300 Loss: 0.10370132327079773\n",
      "Episode: 32400 Loss: 0.13634751737117767\n",
      "Episode: 32500 Loss: 0.10158997774124146\n",
      "Episode: 32600 Loss: 0.12079305201768875\n",
      "Episode: 32700 Loss: 0.12727446854114532\n",
      "Episode: 32800 Loss: 0.10462155938148499\n",
      "Episode: 32900 Loss: 0.13772200047969818\n",
      "Episode: 33000 Loss: 0.10907179862260818\n",
      "Episode: 33100 Loss: 0.10477845370769501\n",
      "Episode: 33200 Loss: 0.14452967047691345\n",
      "Episode: 33300 Loss: 0.13372009992599487\n",
      "Episode: 33400 Loss: 0.10377591848373413\n",
      "Episode: 33500 Loss: 0.10664539784193039\n",
      "Episode: 33600 Loss: 0.10948929190635681\n",
      "Episode: 33700 Loss: 0.13355249166488647\n",
      "Episode: 33800 Loss: 0.10965247452259064\n",
      "Episode: 33900 Loss: 0.1084473505616188\n",
      "Episode: 34000 Loss: 0.1387481838464737\n",
      "Episode: 34100 Loss: 0.10871029645204544\n",
      "Episode: 34200 Loss: 0.11958672106266022\n",
      "Episode: 34300 Loss: 0.107872873544693\n",
      "Episode: 34400 Loss: 0.142054483294487\n",
      "Episode: 34500 Loss: 0.13935670256614685\n",
      "Episode: 34600 Loss: 0.11279504746198654\n",
      "Episode: 34700 Loss: 0.10708490014076233\n",
      "Episode: 34800 Loss: 0.16158875823020935\n",
      "Episode: 34900 Loss: 0.1050562933087349\n",
      "Episode: 35000 Loss: 0.12050207704305649\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.4294133333333333\n",
      "Saving model\n",
      "Episode: 35100 Loss: 0.10864825546741486\n",
      "Episode: 35200 Loss: 0.1179138571023941\n",
      "Episode: 35300 Loss: 0.11435846239328384\n",
      "Episode: 35400 Loss: 0.09988155961036682\n",
      "Episode: 35500 Loss: 0.11284742504358292\n",
      "Episode: 35600 Loss: 0.14190419018268585\n",
      "Episode: 35700 Loss: 0.11476267129182816\n",
      "Episode: 35800 Loss: 0.11780224740505219\n",
      "Episode: 35900 Loss: 0.10736315697431564\n",
      "Episode: 36000 Loss: 0.10815194994211197\n",
      "Episode: 36100 Loss: 0.11521723121404648\n",
      "Episode: 36200 Loss: 0.10368876904249191\n",
      "Episode: 36300 Loss: 0.1504075676202774\n",
      "Episode: 36400 Loss: 0.12791983783245087\n",
      "Episode: 36500 Loss: 0.12105660885572433\n",
      "Episode: 36600 Loss: 0.10685243457555771\n",
      "Episode: 36700 Loss: 0.13871529698371887\n",
      "Episode: 36800 Loss: 0.11943192780017853\n",
      "Episode: 36900 Loss: 0.10742611438035965\n",
      "Episode: 37000 Loss: 0.12321804463863373\n",
      "Episode: 37100 Loss: 0.1017771065235138\n",
      "Episode: 37200 Loss: 0.09459911286830902\n",
      "Episode: 37300 Loss: 0.10164530575275421\n",
      "Episode: 37400 Loss: 0.09787029772996902\n",
      "Episode: 37500 Loss: 0.11100117862224579\n",
      "Episode: 37600 Loss: 0.11850561201572418\n",
      "Episode: 37700 Loss: 0.09230231493711472\n",
      "Episode: 37800 Loss: 0.13983188569545746\n",
      "Episode: 37900 Loss: 0.0984191820025444\n",
      "Episode: 38000 Loss: 0.093582883477211\n",
      "Episode: 38100 Loss: 0.1414758712053299\n",
      "Episode: 38200 Loss: 0.11002042889595032\n",
      "Episode: 38300 Loss: 0.10405507683753967\n",
      "Episode: 38400 Loss: 0.10785962641239166\n",
      "Episode: 38500 Loss: 0.11324349045753479\n",
      "Episode: 38600 Loss: 0.11360594630241394\n",
      "Episode: 38700 Loss: 0.1254003793001175\n",
      "Episode: 38800 Loss: 0.1116725280880928\n",
      "Episode: 38900 Loss: 0.10385092347860336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 39000 Loss: 0.11465155333280563\n",
      "Episode: 39100 Loss: 0.0977013111114502\n",
      "Episode: 39200 Loss: 0.11200759559869766\n",
      "Episode: 39300 Loss: 0.13735944032669067\n",
      "Episode: 39400 Loss: 0.1168680414557457\n",
      "Episode: 39500 Loss: 0.10081189125776291\n",
      "Episode: 39600 Loss: 0.1139603704214096\n",
      "Episode: 39700 Loss: 0.10008884966373444\n",
      "Episode: 39800 Loss: 0.13146212697029114\n",
      "Episode: 39900 Loss: 0.09883448481559753\n",
      "Episode: 40000 Loss: 0.13082613050937653\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.42396\n",
      "Episode: 40100 Loss: 0.12396571040153503\n",
      "Episode: 40200 Loss: 0.1286865770816803\n",
      "Episode: 40300 Loss: 0.11933986097574234\n",
      "Episode: 40400 Loss: 0.11182323098182678\n",
      "Episode: 40500 Loss: 0.1020120307803154\n",
      "Episode: 40600 Loss: 0.10444393008947372\n",
      "Episode: 40700 Loss: 0.11743707209825516\n",
      "Episode: 40800 Loss: 0.10420970618724823\n",
      "Episode: 40900 Loss: 0.1081407442688942\n",
      "Episode: 41000 Loss: 0.09445800632238388\n",
      "Episode: 41100 Loss: 0.10195919871330261\n",
      "Episode: 41200 Loss: 0.10348039865493774\n",
      "Episode: 41300 Loss: 0.10709615796804428\n",
      "Episode: 41400 Loss: 0.1001172587275505\n",
      "Episode: 41500 Loss: 0.13358011841773987\n",
      "Episode: 41600 Loss: 0.13155561685562134\n",
      "Episode: 41700 Loss: 0.11016402393579483\n",
      "Episode: 41800 Loss: 0.11657978594303131\n",
      "Episode: 41900 Loss: 0.11803334206342697\n",
      "Episode: 42000 Loss: 0.14254122972488403\n",
      "Episode: 42100 Loss: 0.10369593650102615\n",
      "Episode: 42200 Loss: 0.11220982670783997\n",
      "Episode: 42300 Loss: 0.11742778867483139\n",
      "Episode: 42400 Loss: 0.10551637411117554\n",
      "Episode: 42500 Loss: 0.09852230548858643\n",
      "Episode: 42600 Loss: 0.10760567337274551\n",
      "Episode: 42700 Loss: 0.09800422936677933\n",
      "Episode: 42800 Loss: 0.11326484382152557\n",
      "Episode: 42900 Loss: 0.09794605523347855\n",
      "Episode: 43000 Loss: 0.12348993122577667\n",
      "Episode: 43100 Loss: 0.108889140188694\n",
      "Episode: 43200 Loss: 0.08898671716451645\n",
      "Episode: 43300 Loss: 0.11178243160247803\n",
      "Episode: 43400 Loss: 0.1347537338733673\n",
      "Episode: 43500 Loss: 0.11314962804317474\n",
      "Episode: 43600 Loss: 0.09897451847791672\n",
      "Episode: 43700 Loss: 0.1251460611820221\n",
      "Episode: 43800 Loss: 0.12559418380260468\n",
      "Episode: 43900 Loss: 0.09254023432731628\n",
      "Episode: 44000 Loss: 0.1017528772354126\n",
      "Episode: 44100 Loss: 0.10739070922136307\n",
      "Episode: 44200 Loss: 0.10791146755218506\n",
      "Episode: 44300 Loss: 0.09176674485206604\n",
      "Episode: 44400 Loss: 0.09232800453901291\n",
      "Episode: 44500 Loss: 0.09671549499034882\n",
      "Episode: 44600 Loss: 0.10597623884677887\n",
      "Episode: 44700 Loss: 0.12602774798870087\n",
      "Episode: 44800 Loss: 0.11468739807605743\n",
      "Episode: 44900 Loss: 0.11081511527299881\n",
      "Episode: 45000 Loss: 0.11397368460893631\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.4318133333333333\n",
      "Saving model\n",
      "Episode: 45100 Loss: 0.12312113493680954\n",
      "Episode: 45200 Loss: 0.10472649335861206\n",
      "Episode: 45300 Loss: 0.08394022285938263\n",
      "Episode: 45400 Loss: 0.09703632444143295\n",
      "Episode: 45500 Loss: 0.125957652926445\n",
      "Episode: 45600 Loss: 0.14849931001663208\n",
      "Episode: 45700 Loss: 0.11886730045080185\n",
      "Episode: 45800 Loss: 0.10262811183929443\n",
      "Episode: 45900 Loss: 0.09340027719736099\n",
      "Episode: 46000 Loss: 0.10291438549757004\n",
      "Episode: 46100 Loss: 0.10704363137483597\n",
      "Episode: 46200 Loss: 0.09154468774795532\n",
      "Episode: 46300 Loss: 0.09920559823513031\n",
      "Episode: 46400 Loss: 0.09780774265527725\n",
      "Episode: 46500 Loss: 0.10063339024782181\n",
      "Episode: 46600 Loss: 0.10107570141553879\n",
      "Episode: 46700 Loss: 0.10038696229457855\n",
      "Episode: 46800 Loss: 0.1096445843577385\n",
      "Episode: 46900 Loss: 0.09020166099071503\n",
      "Episode: 47000 Loss: 0.1245221272110939\n",
      "Episode: 47100 Loss: 0.13079440593719482\n",
      "Episode: 47200 Loss: 0.11879005283117294\n",
      "Episode: 47300 Loss: 0.1127915307879448\n",
      "Episode: 47400 Loss: 0.10280321538448334\n",
      "Episode: 47500 Loss: 0.13336315751075745\n",
      "Episode: 47600 Loss: 0.09309209138154984\n",
      "Episode: 47700 Loss: 0.095743827521801\n",
      "Episode: 47800 Loss: 0.10159608721733093\n",
      "Episode: 47900 Loss: 0.11077748984098434\n",
      "Episode: 48000 Loss: 0.07434078305959702\n",
      "Episode: 48100 Loss: 0.13655690848827362\n",
      "Episode: 48200 Loss: 0.09774893522262573\n",
      "Episode: 48300 Loss: 0.1062479317188263\n",
      "Episode: 48400 Loss: 0.10025683790445328\n",
      "Episode: 48500 Loss: 0.0906224250793457\n",
      "Episode: 48600 Loss: 0.1260889321565628\n",
      "Episode: 48700 Loss: 0.10254635661840439\n",
      "Episode: 48800 Loss: 0.08999794721603394\n",
      "Episode: 48900 Loss: 0.09662602096796036\n",
      "Episode: 49000 Loss: 0.10676944255828857\n",
      "Episode: 49100 Loss: 0.13386571407318115\n",
      "Episode: 49200 Loss: 0.09275651723146439\n",
      "Episode: 49300 Loss: 0.09436190873384476\n",
      "Episode: 49400 Loss: 0.09923328459262848\n",
      "Episode: 49500 Loss: 0.09802699834108353\n",
      "Episode: 49600 Loss: 0.13358744978904724\n",
      "Episode: 49700 Loss: 0.09408535808324814\n",
      "Episode: 49800 Loss: 0.1095491349697113\n",
      "Episode: 49900 Loss: 0.10092668980360031\n",
      "Episode: 50000 Loss: 0.11707901954650879\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.43692000000000003\n",
      "Saving model\n",
      "Episode: 50100 Loss: 0.11140898615121841\n",
      "Episode: 50200 Loss: 0.10245904326438904\n",
      "Episode: 50300 Loss: 0.10618282854557037\n",
      "Episode: 50400 Loss: 0.08764804899692535\n",
      "Episode: 50500 Loss: 0.09761325269937515\n",
      "Episode: 50600 Loss: 0.11800643056631088\n",
      "Episode: 50700 Loss: 0.10842777788639069\n",
      "Episode: 50800 Loss: 0.0892224833369255\n",
      "Episode: 50900 Loss: 0.09748265892267227\n",
      "Episode: 51000 Loss: 0.07937741279602051\n",
      "Episode: 51100 Loss: 0.0704881101846695\n",
      "Episode: 51200 Loss: 0.10633812099695206\n",
      "Episode: 51300 Loss: 0.11383895576000214\n",
      "Episode: 51400 Loss: 0.0911334753036499\n",
      "Episode: 51500 Loss: 0.10922221839427948\n",
      "Episode: 51600 Loss: 0.12414845824241638\n",
      "Episode: 51700 Loss: 0.09427157789468765\n",
      "Episode: 51800 Loss: 0.0930967926979065\n",
      "Episode: 51900 Loss: 0.10090300440788269\n",
      "Episode: 52000 Loss: 0.09837821125984192\n",
      "Episode: 52100 Loss: 0.0834609717130661\n",
      "Episode: 52200 Loss: 0.09881661087274551\n",
      "Episode: 52300 Loss: 0.0848565623164177\n",
      "Episode: 52400 Loss: 0.09117124229669571\n",
      "Episode: 52500 Loss: 0.12266790866851807\n",
      "Episode: 52600 Loss: 0.09482232481241226\n",
      "Episode: 52700 Loss: 0.10788723081350327\n",
      "Episode: 52800 Loss: 0.07876957952976227\n",
      "Episode: 52900 Loss: 0.0766247883439064\n",
      "Episode: 53000 Loss: 0.09490831196308136\n",
      "Episode: 53100 Loss: 0.10662153363227844\n",
      "Episode: 53200 Loss: 0.10260747373104095\n",
      "Episode: 53300 Loss: 0.07668890058994293\n",
      "Episode: 53400 Loss: 0.1026066467165947\n",
      "Episode: 53500 Loss: 0.14576809108257294\n",
      "Episode: 53600 Loss: 0.1301366239786148\n",
      "Episode: 53700 Loss: 0.07349321246147156\n",
      "Episode: 53800 Loss: 0.1065642386674881\n",
      "Episode: 53900 Loss: 0.09505538642406464\n",
      "Episode: 54000 Loss: 0.09110161662101746\n",
      "Episode: 54100 Loss: 0.11172980070114136\n",
      "Episode: 54200 Loss: 0.12474141269922256\n",
      "Episode: 54300 Loss: 0.08037631213665009\n",
      "Episode: 54400 Loss: 0.08651035279035568\n",
      "Episode: 54500 Loss: 0.08412530273199081\n",
      "Episode: 54600 Loss: 0.09312322735786438\n",
      "Episode: 54700 Loss: 0.08977019041776657\n",
      "Episode: 54800 Loss: 0.10751231014728546\n",
      "Episode: 54900 Loss: 0.10785933583974838\n",
      "Episode: 55000 Loss: 0.0957893505692482\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.44082666666666664\n",
      "Saving model\n",
      "Episode: 55100 Loss: 0.08362181484699249\n",
      "Episode: 55200 Loss: 0.08484012633562088\n",
      "Episode: 55300 Loss: 0.08095519244670868\n",
      "Episode: 55400 Loss: 0.10101352632045746\n",
      "Episode: 55500 Loss: 0.11296611279249191\n",
      "Episode: 55600 Loss: 0.1168268620967865\n",
      "Episode: 55700 Loss: 0.09050347656011581\n",
      "Episode: 55800 Loss: 0.07091869413852692\n",
      "Episode: 55900 Loss: 0.13692587614059448\n",
      "Episode: 56000 Loss: 0.10578138381242752\n",
      "Episode: 56100 Loss: 0.11352074891328812\n",
      "Episode: 56200 Loss: 0.1314355731010437\n",
      "Episode: 56300 Loss: 0.08011483401060104\n",
      "Episode: 56400 Loss: 0.08589958399534225\n",
      "Episode: 56500 Loss: 0.12190715223550797\n",
      "Episode: 56600 Loss: 0.11466234177350998\n",
      "Episode: 56700 Loss: 0.0821373462677002\n",
      "Episode: 56800 Loss: 0.08789841830730438\n",
      "Episode: 56900 Loss: 0.12301504611968994\n",
      "Episode: 57000 Loss: 0.10373993217945099\n",
      "Episode: 57100 Loss: 0.07915983349084854\n",
      "Episode: 57200 Loss: 0.08917155861854553\n",
      "Episode: 57300 Loss: 0.08421431481838226\n",
      "Episode: 57400 Loss: 0.12186635285615921\n",
      "Episode: 57500 Loss: 0.08333184570074081\n",
      "Episode: 57600 Loss: 0.10510563105344772\n",
      "Episode: 57700 Loss: 0.08442024141550064\n",
      "Episode: 57800 Loss: 0.08505541831254959\n",
      "Episode: 57900 Loss: 0.08426974713802338\n",
      "Episode: 58000 Loss: 0.08445648849010468\n",
      "Episode: 58100 Loss: 0.10451594740152359\n",
      "Episode: 58200 Loss: 0.09695520997047424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 58300 Loss: 0.11414483934640884\n",
      "Episode: 58400 Loss: 0.10541059076786041\n",
      "Episode: 58500 Loss: 0.1321871280670166\n",
      "Episode: 58600 Loss: 0.09112986922264099\n",
      "Episode: 58700 Loss: 0.10765879601240158\n",
      "Episode: 58800 Loss: 0.14407476782798767\n",
      "Episode: 58900 Loss: 0.0810224786400795\n",
      "Episode: 59000 Loss: 0.09004433453083038\n",
      "Episode: 59100 Loss: 0.08513959497213364\n",
      "Episode: 59200 Loss: 0.11329155415296555\n",
      "Episode: 59300 Loss: 0.09473826736211777\n",
      "Episode: 59400 Loss: 0.08527130633592606\n",
      "Episode: 59500 Loss: 0.07199623435735703\n",
      "Episode: 59600 Loss: 0.10287430137395859\n",
      "Episode: 59700 Loss: 0.08081746101379395\n",
      "Episode: 59800 Loss: 0.10817600786685944\n",
      "Episode: 59900 Loss: 0.10789787024259567\n",
      "Episode: 60000 Loss: 0.0900472104549408\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.44552\n",
      "Saving model\n",
      "Episode: 60100 Loss: 0.07124406099319458\n",
      "Episode: 60200 Loss: 0.0971679612994194\n",
      "Episode: 60300 Loss: 0.07859226316213608\n",
      "Episode: 60400 Loss: 0.11741618067026138\n",
      "Episode: 60500 Loss: 0.14838019013404846\n",
      "Episode: 60600 Loss: 0.08468583971261978\n",
      "Episode: 60700 Loss: 0.07833254337310791\n",
      "Episode: 60800 Loss: 0.08719740062952042\n",
      "Episode: 60900 Loss: 0.08628857135772705\n",
      "Episode: 61000 Loss: 0.08888940513134003\n",
      "Episode: 61100 Loss: 0.09614351391792297\n",
      "Episode: 61200 Loss: 0.10191496461629868\n",
      "Episode: 61300 Loss: 0.08747042715549469\n",
      "Episode: 61400 Loss: 0.08047986775636673\n",
      "Episode: 61500 Loss: 0.11986209452152252\n",
      "Episode: 61600 Loss: 0.1177411675453186\n",
      "Episode: 61700 Loss: 0.10169253498315811\n",
      "Episode: 61800 Loss: 0.1314181387424469\n",
      "Episode: 61900 Loss: 0.11580299586057663\n",
      "Episode: 62000 Loss: 0.09654446691274643\n",
      "Episode: 62100 Loss: 0.09889686852693558\n",
      "Episode: 62200 Loss: 0.08756173402070999\n",
      "Episode: 62300 Loss: 0.1210835799574852\n",
      "Episode: 62400 Loss: 0.09384860098361969\n",
      "Episode: 62500 Loss: 0.11922857910394669\n",
      "Episode: 62600 Loss: 0.08154436200857162\n",
      "Episode: 62700 Loss: 0.06565529108047485\n",
      "Episode: 62800 Loss: 0.11238648742437363\n",
      "Episode: 62900 Loss: 0.09469769150018692\n",
      "Episode: 63000 Loss: 0.07395894080400467\n",
      "Episode: 63100 Loss: 0.08720576018095016\n",
      "Episode: 63200 Loss: 0.0771467313170433\n",
      "Episode: 63300 Loss: 0.10631279647350311\n",
      "Episode: 63400 Loss: 0.08299888670444489\n",
      "Episode: 63500 Loss: 0.07028628885746002\n",
      "Episode: 63600 Loss: 0.11482185870409012\n",
      "Episode: 63700 Loss: 0.10269658267498016\n",
      "Episode: 63800 Loss: 0.10067076981067657\n",
      "Episode: 63900 Loss: 0.11215093731880188\n",
      "Episode: 64000 Loss: 0.12527428567409515\n",
      "Episode: 64100 Loss: 0.07075316458940506\n",
      "Episode: 64200 Loss: 0.09324672073125839\n",
      "Episode: 64300 Loss: 0.10498502850532532\n",
      "Episode: 64400 Loss: 0.08181744813919067\n",
      "Episode: 64500 Loss: 0.08999612182378769\n",
      "Episode: 64600 Loss: 0.10847784578800201\n",
      "Episode: 64700 Loss: 0.10354815423488617\n",
      "Episode: 64800 Loss: 0.0887652188539505\n",
      "Episode: 64900 Loss: 0.070110023021698\n",
      "Episode: 65000 Loss: 0.10574112832546234\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.44024\n",
      "Episode: 65100 Loss: 0.11820894479751587\n",
      "Episode: 65200 Loss: 0.09091278910636902\n",
      "Episode: 65300 Loss: 0.10455058515071869\n",
      "Episode: 65400 Loss: 0.08591128885746002\n",
      "Episode: 65500 Loss: 0.10489305853843689\n",
      "Episode: 65600 Loss: 0.11047469824552536\n",
      "Episode: 65700 Loss: 0.1090347021818161\n",
      "Episode: 65800 Loss: 0.0788872092962265\n",
      "Episode: 65900 Loss: 0.10650146752595901\n",
      "Episode: 66000 Loss: 0.07834721356630325\n",
      "Episode: 66100 Loss: 0.08308639377355576\n",
      "Episode: 66200 Loss: 0.0932048037648201\n",
      "Episode: 66300 Loss: 0.11537554115056992\n",
      "Episode: 66400 Loss: 0.06620565801858902\n",
      "Episode: 66500 Loss: 0.07385128736495972\n",
      "Episode: 66600 Loss: 0.08174437284469604\n",
      "Episode: 66700 Loss: 0.07861658185720444\n",
      "Episode: 66800 Loss: 0.0603373758494854\n",
      "Episode: 66900 Loss: 0.08564528077840805\n",
      "Episode: 67000 Loss: 0.10332885384559631\n",
      "Episode: 67100 Loss: 0.07701965421438217\n",
      "Episode: 67200 Loss: 0.10062452405691147\n",
      "Episode: 67300 Loss: 0.08616901934146881\n",
      "Episode: 67400 Loss: 0.09433742612600327\n",
      "Episode: 67500 Loss: 0.13715070486068726\n",
      "Episode: 67600 Loss: 0.07913484424352646\n",
      "Episode: 67700 Loss: 0.1107955127954483\n",
      "Episode: 67800 Loss: 0.10031582415103912\n",
      "Episode: 67900 Loss: 0.11015238612890244\n",
      "Episode: 68000 Loss: 0.07141466438770294\n",
      "Episode: 68100 Loss: 0.07232201844453812\n",
      "Episode: 68200 Loss: 0.08976808935403824\n",
      "Episode: 68300 Loss: 0.06990167498588562\n",
      "Episode: 68400 Loss: 0.08105199038982391\n",
      "Episode: 68500 Loss: 0.0999159887433052\n",
      "Episode: 68600 Loss: 0.10724472254514694\n",
      "Episode: 68700 Loss: 0.09754113107919693\n",
      "Episode: 68800 Loss: 0.08694926649332047\n",
      "Episode: 68900 Loss: 0.11601671576499939\n",
      "Episode: 69000 Loss: 0.10676150768995285\n",
      "Episode: 69100 Loss: 0.08539970219135284\n",
      "Episode: 69200 Loss: 0.07740376144647598\n",
      "Episode: 69300 Loss: 0.07388053089380264\n",
      "Episode: 69400 Loss: 0.11788823455572128\n",
      "Episode: 69500 Loss: 0.07446049153804779\n",
      "Episode: 69600 Loss: 0.08451174199581146\n",
      "Episode: 69700 Loss: 0.0809224396944046\n",
      "Episode: 69800 Loss: 0.08072125911712646\n",
      "Episode: 69900 Loss: 0.08405432850122452\n",
      "Episode: 70000 Loss: 0.09614000469446182\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.442\n",
      "Episode: 70100 Loss: 0.08833368867635727\n",
      "Episode: 70200 Loss: 0.08436136692762375\n",
      "Episode: 70300 Loss: 0.1314530223608017\n",
      "Episode: 70400 Loss: 0.07183223962783813\n",
      "Episode: 70500 Loss: 0.08494644612073898\n",
      "Episode: 70600 Loss: 0.08308672904968262\n",
      "Episode: 70700 Loss: 0.08332469314336777\n",
      "Episode: 70800 Loss: 0.09644877165555954\n",
      "Episode: 70900 Loss: 0.07752013951539993\n",
      "Episode: 71000 Loss: 0.06781632453203201\n",
      "Episode: 71100 Loss: 0.10465174168348312\n",
      "Episode: 71200 Loss: 0.07125172019004822\n",
      "Episode: 71300 Loss: 0.06465842574834824\n",
      "Episode: 71400 Loss: 0.09782058000564575\n",
      "Episode: 71500 Loss: 0.07775118201971054\n",
      "Episode: 71600 Loss: 0.10872851312160492\n",
      "Episode: 71700 Loss: 0.07077284157276154\n",
      "Episode: 71800 Loss: 0.06322433054447174\n",
      "Episode: 71900 Loss: 0.08184150606393814\n",
      "Episode: 72000 Loss: 0.08787915855646133\n",
      "Episode: 72100 Loss: 0.10795067995786667\n",
      "Episode: 72200 Loss: 0.11529691517353058\n",
      "Episode: 72300 Loss: 0.10545860975980759\n",
      "Episode: 72400 Loss: 0.10320967435836792\n",
      "Episode: 72500 Loss: 0.10169892013072968\n",
      "Episode: 72600 Loss: 0.05677293241024017\n",
      "Episode: 72700 Loss: 0.07694525271654129\n",
      "Episode: 72800 Loss: 0.08426878601312637\n",
      "Episode: 72900 Loss: 0.114281065762043\n",
      "Episode: 73000 Loss: 0.10677532851696014\n",
      "Episode: 73100 Loss: 0.0949149951338768\n",
      "Episode: 73200 Loss: 0.10236108303070068\n",
      "Episode: 73300 Loss: 0.10274595022201538\n",
      "Episode: 73400 Loss: 0.06672703474760056\n",
      "Episode: 73500 Loss: 0.11068141460418701\n",
      "Episode: 73600 Loss: 0.0777895376086235\n",
      "Episode: 73700 Loss: 0.09931425005197525\n",
      "Episode: 73800 Loss: 0.11057131737470627\n",
      "Episode: 73900 Loss: 0.1059250459074974\n",
      "Episode: 74000 Loss: 0.09601782262325287\n",
      "Episode: 74100 Loss: 0.11813344061374664\n",
      "Episode: 74200 Loss: 0.0978209525346756\n",
      "Episode: 74300 Loss: 0.11265872418880463\n",
      "Episode: 74400 Loss: 0.07223770022392273\n",
      "Episode: 74500 Loss: 0.09564737230539322\n",
      "Episode: 74600 Loss: 0.09235838800668716\n",
      "Episode: 74700 Loss: 0.10848572105169296\n",
      "Episode: 74800 Loss: 0.08171534538269043\n",
      "Episode: 74900 Loss: 0.09739013761281967\n",
      "Episode: 75000 Loss: 0.09689359366893768\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.442\n",
      "Episode: 75100 Loss: 0.07323084771633148\n",
      "Episode: 75200 Loss: 0.07355070114135742\n",
      "Episode: 75300 Loss: 0.09247204661369324\n",
      "Episode: 75400 Loss: 0.08569706231355667\n",
      "Episode: 75500 Loss: 0.10380497574806213\n",
      "Episode: 75600 Loss: 0.09734535962343216\n",
      "Episode: 75700 Loss: 0.08362333476543427\n",
      "Episode: 75800 Loss: 0.08561211824417114\n",
      "Episode: 75900 Loss: 0.09616757929325104\n",
      "Episode: 76000 Loss: 0.07613010704517365\n",
      "Episode: 76100 Loss: 0.09313394874334335\n",
      "Episode: 76200 Loss: 0.12786231935024261\n",
      "Episode: 76300 Loss: 0.08898159116506577\n",
      "Episode: 76400 Loss: 0.0823628231883049\n",
      "Episode: 76500 Loss: 0.09546787291765213\n",
      "Episode: 76600 Loss: 0.07211042940616608\n",
      "Episode: 76700 Loss: 0.07975569367408752\n",
      "Episode: 76800 Loss: 0.1064455583691597\n",
      "Episode: 76900 Loss: 0.11957396566867828\n",
      "Episode: 77000 Loss: 0.07914843410253525\n",
      "Episode: 77100 Loss: 0.09112357348203659\n",
      "Episode: 77200 Loss: 0.08286550641059875\n",
      "Episode: 77300 Loss: 0.08393138647079468\n",
      "Episode: 77400 Loss: 0.11368615925312042\n",
      "Episode: 77500 Loss: 0.09696932882070541\n",
      "Episode: 77600 Loss: 0.07391323149204254\n",
      "Episode: 77700 Loss: 0.06261716783046722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 77800 Loss: 0.08294432610273361\n",
      "Episode: 77900 Loss: 0.08510445058345795\n",
      "Episode: 78000 Loss: 0.07954765111207962\n",
      "Episode: 78100 Loss: 0.06435859948396683\n",
      "Episode: 78200 Loss: 0.10014348477125168\n",
      "Episode: 78300 Loss: 0.09211694449186325\n",
      "Episode: 78400 Loss: 0.058398183435201645\n",
      "Episode: 78500 Loss: 0.08468029648065567\n",
      "Episode: 78600 Loss: 0.08199609071016312\n",
      "Episode: 78700 Loss: 0.1035439670085907\n",
      "Episode: 78800 Loss: 0.10910926014184952\n",
      "Episode: 78900 Loss: 0.10390175133943558\n",
      "Episode: 79000 Loss: 0.08561650663614273\n",
      "Episode: 79100 Loss: 0.08996885269880295\n",
      "Episode: 79200 Loss: 0.0949498787522316\n",
      "Episode: 79300 Loss: 0.08980295062065125\n",
      "Episode: 79400 Loss: 0.0617012158036232\n",
      "Episode: 79500 Loss: 0.07126345485448837\n",
      "Episode: 79600 Loss: 0.09499789774417877\n",
      "Episode: 79700 Loss: 0.06762786209583282\n",
      "Episode: 79800 Loss: 0.0917576402425766\n",
      "Episode: 79900 Loss: 0.10007762908935547\n",
      "Episode: 80000 Loss: 0.0838574543595314\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.4397466666666667\n",
      "Episode: 80100 Loss: 0.07149022072553635\n",
      "Episode: 80200 Loss: 0.11524540185928345\n",
      "Episode: 80300 Loss: 0.08501783013343811\n",
      "Episode: 80400 Loss: 0.09273377805948257\n",
      "Episode: 80500 Loss: 0.08367688953876495\n",
      "Episode: 80600 Loss: 0.09294253587722778\n",
      "Episode: 80700 Loss: 0.07405529916286469\n",
      "Episode: 80800 Loss: 0.07496126741170883\n",
      "Episode: 80900 Loss: 0.07418207079172134\n",
      "Episode: 81000 Loss: 0.07038278877735138\n",
      "Episode: 81100 Loss: 0.07835317403078079\n",
      "Episode: 81200 Loss: 0.08535841107368469\n",
      "Episode: 81300 Loss: 0.1292540431022644\n",
      "Episode: 81400 Loss: 0.07528912276029587\n",
      "Episode: 81500 Loss: 0.10777658969163895\n",
      "Episode: 81600 Loss: 0.07685332745313644\n",
      "Episode: 81700 Loss: 0.04927873611450195\n",
      "Episode: 81800 Loss: 0.0946395993232727\n",
      "Episode: 81900 Loss: 0.07360667735338211\n",
      "Episode: 82000 Loss: 0.09031335264444351\n",
      "Episode: 82100 Loss: 0.08597854524850845\n",
      "Episode: 82200 Loss: 0.08436696976423264\n",
      "Episode: 82300 Loss: 0.08475936949253082\n",
      "Episode: 82400 Loss: 0.08242687582969666\n",
      "Episode: 82500 Loss: 0.09029010683298111\n",
      "Episode: 82600 Loss: 0.0932702049612999\n",
      "Episode: 82700 Loss: 0.0718824490904808\n",
      "Episode: 82800 Loss: 0.06325144320726395\n",
      "Episode: 82900 Loss: 0.07824233919382095\n",
      "Episode: 83000 Loss: 0.08630051463842392\n",
      "Episode: 83100 Loss: 0.10755719244480133\n",
      "Episode: 83200 Loss: 0.08053280413150787\n",
      "Episode: 83300 Loss: 0.08037770539522171\n",
      "Episode: 83400 Loss: 0.08166852593421936\n",
      "Episode: 83500 Loss: 0.10732812434434891\n",
      "Episode: 83600 Loss: 0.06427323073148727\n",
      "Episode: 83700 Loss: 0.09849551320075989\n",
      "Episode: 83800 Loss: 0.08454610407352448\n",
      "Episode: 83900 Loss: 0.08459312468767166\n",
      "Episode: 84000 Loss: 0.07194792479276657\n",
      "Episode: 84100 Loss: 0.10281307250261307\n",
      "Episode: 84200 Loss: 0.062261104583740234\n",
      "Episode: 84300 Loss: 0.08013487607240677\n",
      "Episode: 84400 Loss: 0.07922215014696121\n",
      "Episode: 84500 Loss: 0.08703651279211044\n",
      "Episode: 84600 Loss: 0.08044879883527756\n",
      "Episode: 84700 Loss: 0.09725832939147949\n",
      "Episode: 84800 Loss: 0.07873295247554779\n",
      "Episode: 84900 Loss: 0.0871201753616333\n",
      "Episode: 85000 Loss: 0.07065417617559433\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.45061333333333337\n",
      "Saving model\n",
      "Episode: 85100 Loss: 0.07396489381790161\n",
      "Episode: 85200 Loss: 0.1071026623249054\n",
      "Episode: 85300 Loss: 0.058491919189691544\n",
      "Episode: 85400 Loss: 0.06919179111719131\n",
      "Episode: 85500 Loss: 0.08678486943244934\n",
      "Episode: 85600 Loss: 0.08282748609781265\n",
      "Episode: 85700 Loss: 0.0671856701374054\n",
      "Episode: 85800 Loss: 0.12212510406970978\n",
      "Episode: 85900 Loss: 0.08663715422153473\n",
      "Episode: 86000 Loss: 0.0875125303864479\n",
      "Episode: 86100 Loss: 0.05676520615816116\n",
      "Episode: 86200 Loss: 0.11470463126897812\n",
      "Episode: 86300 Loss: 0.060305505990982056\n",
      "Episode: 86400 Loss: 0.09825178980827332\n",
      "Episode: 86500 Loss: 0.0965188667178154\n",
      "Episode: 86600 Loss: 0.08296339213848114\n",
      "Episode: 86700 Loss: 0.11492319405078888\n",
      "Episode: 86800 Loss: 0.08912646770477295\n",
      "Episode: 86900 Loss: 0.06846334040164948\n",
      "Episode: 87000 Loss: 0.097727470099926\n",
      "Episode: 87100 Loss: 0.1005329117178917\n",
      "Episode: 87200 Loss: 0.0911150649189949\n",
      "Episode: 87300 Loss: 0.08500265330076218\n",
      "Episode: 87400 Loss: 0.0749378576874733\n",
      "Episode: 87500 Loss: 0.07279514521360397\n",
      "Episode: 87600 Loss: 0.115276038646698\n",
      "Episode: 87700 Loss: 0.09376057237386703\n",
      "Episode: 87800 Loss: 0.07087749242782593\n",
      "Episode: 87900 Loss: 0.06764412671327591\n",
      "Episode: 88000 Loss: 0.08429879695177078\n",
      "Episode: 88100 Loss: 0.07509568333625793\n",
      "Episode: 88200 Loss: 0.09371288865804672\n",
      "Episode: 88300 Loss: 0.09491006284952164\n",
      "Episode: 88400 Loss: 0.05579662695527077\n",
      "Episode: 88500 Loss: 0.07939649373292923\n",
      "Episode: 88600 Loss: 0.0958934798836708\n",
      "Episode: 88700 Loss: 0.07197411358356476\n",
      "Episode: 88800 Loss: 0.06107978895306587\n",
      "Episode: 88900 Loss: 0.0844532772898674\n",
      "Episode: 89000 Loss: 0.07604559510946274\n",
      "Episode: 89100 Loss: 0.083722323179245\n",
      "Episode: 89200 Loss: 0.08291663974523544\n",
      "Episode: 89300 Loss: 0.08734183013439178\n",
      "Episode: 89400 Loss: 0.06156981736421585\n",
      "Episode: 89500 Loss: 0.07416091859340668\n",
      "Episode: 89600 Loss: 0.09459400177001953\n",
      "Episode: 89700 Loss: 0.10741941630840302\n",
      "Episode: 89800 Loss: 0.09268373250961304\n",
      "Episode: 89900 Loss: 0.09032509475946426\n",
      "Episode: 90000 Loss: 0.09722410887479782\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.44581333333333334\n",
      "Episode: 90100 Loss: 0.05154193192720413\n",
      "Episode: 90200 Loss: 0.07449442148208618\n",
      "Episode: 90300 Loss: 0.1047811508178711\n",
      "Episode: 90400 Loss: 0.09376133233308792\n",
      "Episode: 90500 Loss: 0.08983553946018219\n",
      "Episode: 90600 Loss: 0.09553653746843338\n",
      "Episode: 90700 Loss: 0.09636134654283524\n",
      "Episode: 90800 Loss: 0.06767565757036209\n",
      "Episode: 90900 Loss: 0.0848202332854271\n",
      "Episode: 91000 Loss: 0.07046522200107574\n",
      "Episode: 91100 Loss: 0.0989956185221672\n",
      "Episode: 91200 Loss: 0.09349294751882553\n",
      "Episode: 91300 Loss: 0.08120661973953247\n",
      "Episode: 91400 Loss: 0.07904117554426193\n",
      "Episode: 91500 Loss: 0.07505203783512115\n",
      "Episode: 91600 Loss: 0.0587623193860054\n",
      "Episode: 91700 Loss: 0.07700368762016296\n",
      "Episode: 91800 Loss: 0.06139393150806427\n",
      "Episode: 91900 Loss: 0.11122775077819824\n",
      "Episode: 92000 Loss: 0.07341233640909195\n",
      "Episode: 92100 Loss: 0.07453151792287827\n",
      "Episode: 92200 Loss: 0.10493110865354538\n",
      "Episode: 92300 Loss: 0.11036598682403564\n",
      "Episode: 92400 Loss: 0.078692227602005\n",
      "Episode: 92500 Loss: 0.07910089194774628\n",
      "Episode: 92600 Loss: 0.12104624509811401\n",
      "Episode: 92700 Loss: 0.08044734597206116\n",
      "Episode: 92800 Loss: 0.0707177221775055\n",
      "Episode: 92900 Loss: 0.10908126085996628\n",
      "Episode: 93000 Loss: 0.10615348070859909\n",
      "Episode: 93100 Loss: 0.10045512765645981\n",
      "Episode: 93200 Loss: 0.06219977140426636\n",
      "Episode: 93300 Loss: 0.07599092274904251\n",
      "Episode: 93400 Loss: 0.09782035648822784\n",
      "Episode: 93500 Loss: 0.10878752917051315\n",
      "Episode: 93600 Loss: 0.11615712195634842\n",
      "Episode: 93700 Loss: 0.07138407230377197\n",
      "Episode: 93800 Loss: 0.08915871381759644\n",
      "Episode: 93900 Loss: 0.07798558473587036\n",
      "Episode: 94000 Loss: 0.09606967866420746\n",
      "Episode: 94100 Loss: 0.09059646725654602\n",
      "Episode: 94200 Loss: 0.08321261405944824\n",
      "Episode: 94300 Loss: 0.06778571754693985\n",
      "Episode: 94400 Loss: 0.08893875777721405\n",
      "Episode: 94500 Loss: 0.10659609735012054\n",
      "Episode: 94600 Loss: 0.10874003171920776\n",
      "Episode: 94700 Loss: 0.09261811524629593\n",
      "Episode: 94800 Loss: 0.06491820514202118\n",
      "Episode: 94900 Loss: 0.05150183290243149\n",
      "Episode: 95000 Loss: 0.0946224257349968\n",
      "Scoring on validation set...\n",
      "Validation set accuracy: 0.4431333333333333\n",
      "Episode: 95100 Loss: 0.06797724962234497\n",
      "Episode: 95200 Loss: 0.08034960180521011\n",
      "Episode: 95300 Loss: 0.08294457942247391\n",
      "Episode: 95400 Loss: 0.10516247898340225\n",
      "Episode: 95500 Loss: 0.09480553865432739\n",
      "Episode: 95600 Loss: 0.11828523874282837\n",
      "Episode: 95700 Loss: 0.10530702024698257\n",
      "Episode: 95800 Loss: 0.06790495663881302\n",
      "Episode: 95900 Loss: 0.06929605454206467\n",
      "Episode: 96000 Loss: 0.08926182240247726\n",
      "Episode: 96100 Loss: 0.07374396920204163\n",
      "Episode: 96200 Loss: 0.07243199646472931\n",
      "Episode: 96300 Loss: 0.06325753778219223\n",
      "Episode: 96400 Loss: 0.10625281929969788\n",
      "Episode: 96500 Loss: 0.058243561536073685\n",
      "Episode: 96600 Loss: 0.0711900144815445\n",
      "Episode: 96700 Loss: 0.08165533095598221\n",
      "Episode: 96800 Loss: 0.07932017743587494\n",
      "Episode: 96900 Loss: 0.08128533512353897\n",
      "Episode: 97000 Loss: 0.06898894906044006\n",
      "Episode: 97100 Loss: 0.08737297356128693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 97200 Loss: 0.06025364622473717\n",
      "Episode: 97300 Loss: 0.10015638172626495\n",
      "Episode: 97400 Loss: 0.0840829461812973\n",
      "Episode: 97500 Loss: 0.10319524258375168\n",
      "Episode: 97600 Loss: 0.06907015293836594\n",
      "Episode: 97700 Loss: 0.10065454244613647\n",
      "Episode: 97800 Loss: 0.0638587549328804\n",
      "Episode: 97900 Loss: 0.10316804051399231\n",
      "Episode: 98000 Loss: 0.08068301528692245\n",
      "Episode: 98100 Loss: 0.07418342679738998\n",
      "Episode: 98200 Loss: 0.05160099267959595\n",
      "Episode: 98300 Loss: 0.0761108323931694\n",
      "Episode: 98400 Loss: 0.08698144555091858\n",
      "Episode: 98500 Loss: 0.08840814977884293\n",
      "Episode: 98600 Loss: 0.10412349551916122\n",
      "Episode: 98700 Loss: 0.061697788536548615\n",
      "Episode: 98800 Loss: 0.06349963694810867\n",
      "Episode: 98900 Loss: 0.09987089037895203\n",
      "Episode: 99000 Loss: 0.09823610633611679\n",
      "Episode: 99100 Loss: 0.0719592347741127\n",
      "Episode: 99200 Loss: 0.06052582338452339\n",
      "Episode: 99300 Loss: 0.06018393114209175\n",
      "Episode: 99400 Loss: 0.108957938849926\n",
      "Episode: 99500 Loss: 0.0778040662407875\n",
      "Episode: 99600 Loss: 0.07898913323879242\n",
      "Episode: 99700 Loss: 0.07367753237485886\n",
      "Episode: 99800 Loss: 0.09298522770404816\n",
      "Episode: 99900 Loss: 0.11198912560939789\n",
      "Episode: 100000 Loss: 0.06573464721441269\n"
     ]
    }
   ],
   "source": [
    "rn.train( print_on=100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cohorts SotA result for clothing category with binary classification was 0.683. Clearly we are not beating this, although thats not surprising. With the amount of data we have, and the interaction between other labels (e.g. sleeve length, clothing pattern, etc) that we are ignoring, it is not surprising that a standard resnet classifier outperforms the relational net. We would expect the ~45% result we obtain here as the original paper achieved around ~50% on a subset of imagenet. The two are generally similar in that they contain a lot of background noise per image and the comonalities between classes are more nuanced.\n",
    "\n",
    "Having said that, with a proper segmentation model, the relational net can focus more on shape/colour/etc of the clothes without being distracted by background and we may well achieve better results, more comparable to the omniglot benchmark which achieves ~99% accuracy. This could also be true for the resnet classifier however so it remains to be seen what the better approach is.\n",
    "\n",
    "The real value of relational nets however is in few-shot analysis. Since we don't have the similar.ai dataset at the time of writing we are still unsure of what approaches are even available to us and it may be the case that we _have_ to use relational nets.\n",
    "\n",
    "We may also be able to improve accuracy, as mentioned above, with separate heads for each label and loss interaction terms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
