{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_label.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHsMBaABXuGS",
        "colab_type": "text"
      },
      "source": [
        "# ProductNet: Categorize products using image(s) and text descriptors\n",
        "## Jen Sheng Wong and Kartik Nanda (Cohort 14)\n",
        "Based on following paper: https://arxiv.org/pdf/1904.09037.pdf\n",
        "\n",
        "## Problem Statement:\n",
        "Relates primarily to products on retail/marketplace sites such as Amazon. The problem deals with 3 main aspects:\n",
        "* Trying to categorize products. Number of categories ~5000 (using the Google taxonomy: https://github.com/fellowship/platform-demos3/blob/master/ProductNet/taxonomy-with-ids.en-US.xls)\n",
        "* Product has images (1 or more)\n",
        "* Product has text - title, description, keywords\n",
        "\n",
        "Possible end-problems to solve: \n",
        "a) Find the category, given product images and user provided text description. \n",
        "b) Find mis-categorized products\n",
        "\n",
        "## Dataset: \n",
        "Products for sale on Amazon; product details from 1996 through 2014, by Prof. McAuley at UCSD\n",
        "Citations:\n",
        "* R. He, J. McAuley. Modeling the visual evolution of fashion trends with one-class collaborative filtering. WWW, 2016\n",
        "* J. McAuley, C. Targett, J. Shi, A. van den Hengel. Image-based recommendations on styles and substitutes. SIGIR, 2015\n",
        "\n",
        "## Dataset Storage:\n",
        "* Used Google storage bucket located at gs://platform-ai-research/datasets/ProductNet/\n",
        "* Also used Jen Sheng's Google Drive for intermediate files, images etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj2Rh8RLXuGU",
        "colab_type": "text"
      },
      "source": [
        "## Generating labels for the dataset\n",
        "The dataset has three text fields - categories, description and title. Any and all of these can be used to generate labels for the dataset. \n",
        "\n",
        "The first attempt involved using the 'categories' entry in the dataset itself as the label. This results in ~90,000 unique labels. The error_rates working with this were in the range of 98%.\n",
        "\n",
        "The second attempt extracted the label from the categories by picking the first category entry in the categories column. This reduced the number of unique labels to ~40 for a smaller sampled dataset (10k instead of 5.7 million). error-rate was ~70%\n",
        "\n",
        "Third attempt generates labels by mapping the categories data to google taxonomy entries. We then use these new labels to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LHsRqO0XuGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fastai\n",
        "!pip install pyarrow\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from fastai.vision import *\n",
        "from fastai.metrics import error_rate, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVnaza8GXuGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the cleaned dataset from the GS bucket\n",
        "# Set the file name - this is the output file from step1 (workbook_1)\n",
        "file_name = 'metadata_clean_0513'\n",
        "gs_path = 'platform-ai-research/datasets/ProductNet/'     # location of the bucket\n",
        "\n",
        "# set local to False if running on Colab\n",
        "local = False      # this needs to be automated (how?)\n",
        "\n",
        "# Reading in the datafile\n",
        "exists = os.path.isfile(file_name)\n",
        "if (not exists):\n",
        "    if (not local):\n",
        "        # Login to access the GS bucket\n",
        "        from google.colab import auth\n",
        "        auth.authenticate_user()\n",
        "\n",
        "        # Copy the datafile to the Colab local dir\n",
        "        try:\n",
        "            remote_file = gs_path + file_name\n",
        "            !gsutil cp gs://{remote_file} {file_name}\n",
        "        except Exception as e:\n",
        "            print('File Does Not Exist')\n",
        "            sys.exit()\n",
        "                \n",
        "    else: \n",
        "        print('File Does Not Exist')\n",
        "        sys.exit()\n",
        "\n",
        "# Read in the file\n",
        "df = pd.read_feather(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1aiON4dXuGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e6cfcaae-5803-4952-96e1-3a61a343dcc2"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5607160 entries, 0 to 5607159\n",
            "Data columns (total 7 columns):\n",
            "index          int64\n",
            "asin           object\n",
            "categories     object\n",
            "description    object\n",
            "imUrl          object\n",
            "title          object\n",
            "label          object\n",
            "dtypes: int64(1), object(6)\n",
            "memory usage: 299.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l49Ifgf7XuGh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ca94873b-c2cc-40f0-87e4-ba3f6dc4b83d"
      },
      "source": [
        "df = df[['asin', 'categories']]    # keep only the asin and categories columns\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asin</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000037214</td>\n",
              "      <td>Clothing, Shoes &amp; Jewelry, Girls, Clothing, Sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000032069</td>\n",
              "      <td>Sports &amp; Outdoors, Other Sports, Dance, Clothi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000031909</td>\n",
              "      <td>Sports &amp; Outdoors, Other Sports, Dance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0000032034</td>\n",
              "      <td>Sports &amp; Outdoors, Other Sports, Dance, Clothi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000031852</td>\n",
              "      <td>Sports &amp; Outdoors, Other Sports, Dance</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         asin                                         categories\n",
              "0  0000037214  Clothing, Shoes & Jewelry, Girls, Clothing, Sh...\n",
              "1  0000032069  Sports & Outdoors, Other Sports, Dance, Clothi...\n",
              "2  0000031909             Sports & Outdoors, Other Sports, Dance\n",
              "3  0000032034  Sports & Outdoors, Other Sports, Dance, Clothi...\n",
              "4  0000031852             Sports & Outdoors, Other Sports, Dance"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEpugCU9YNzd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9a997d33-3747-4c61-9cfd-26b03bd21063"
      },
      "source": [
        "# Using the Universal Sentence Encoder to map categories to google taxonomy labels\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "hub_embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0515 20:27:48.638885 140305067239296 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeqEH7b6YmZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate the embeddings for various text input\n",
        "def get_use_matrix(s):\n",
        "    embeddings = hub_embed(s)\n",
        "\n",
        "    with tf.Session() as session:\n",
        "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "        xtrain_embeddings = session.run(embeddings)\n",
        "        \n",
        "    return xtrain_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPBneGAnb1sk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "91324142-56e7-4f6f-c5fa-8f54ef584a8a"
      },
      "source": [
        "# Generate the embeddings for the taxonomy - the labels we are trying to map to\n",
        "# Read in the taxonomy file from the bucket\n",
        "file_name = 'taxonomy-with-ids.en-US.xls'\n",
        "\n",
        "exists = os.path.isfile(file_name)\n",
        "if (not exists):\n",
        "    try:\n",
        "        remote_file = gs_path + file_name\n",
        "        !gsutil cp gs://{remote_file} .\n",
        "    except Exception as e:\n",
        "            print('File Does Not Exist')\n",
        "            sys.exit()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://platform-ai-research/datasets/ProductNet/taxonomy-with-ids.en-US.xls...\n",
            "- [1 files][603.0 KiB/603.0 KiB]                                                \n",
            "Operation completed over 1 objects/603.0 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXDt41HEcYaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the taxonomy file\n",
        "taxo = pd.read_excel('taxonomy-with-ids.en-US.xls')\n",
        "\n",
        "taxo = taxo.fillna('')\n",
        "\n",
        "# get columns. Different columns provide a different depth into the taxonomy.\n",
        "# For example column 1 would be top level, and has few (31) categories\n",
        "# Column 2 is one level deeper, and has 192 labels, column 3 is deeper still, and so on\n",
        "level_2 = taxo.iloc[:, 2].unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV5uosaKdtyh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbb7a4ca-59d0-441d-9257-3a4b705dd39c"
      },
      "source": [
        "taxo_embeddings = get_use_matrix(level_2)\n",
        "taxo_embeddings.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(192, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1bpsujRrnvm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "outputId": "f804bd49-59c5-4f39-f395-1ed0a1b824e9"
      },
      "source": [
        "# Can't run the entire dataset on colab (not enough RAM) so trying in pieces\n",
        "pieces = 1000; n = int(df.shape[0]/pieces)\n",
        "df3 = pd.DataFrame()\n",
        "\n",
        "for i in range(0,100,1):\n",
        "    df2 = None; classes_embeddings_piece = None\n",
        "    df2 = df[i*n:(i+1)*n-1].copy()\n",
        "    \n",
        "    # generate embeddings for the categories entries\n",
        "    s = df2.categories.values\n",
        "    classes_embeddings_piece = get_use_matrix(s)\n",
        "    \n",
        "    # We now have the embeddings for both the categories data (the source) and the taxonomy (the target)\n",
        "    # Need to map the source (categories embeddings) to the target.\n",
        "    # the label is the embedding in the taxonomy that is \"closest\" to the categories embedding\n",
        "\n",
        "    # Calculate the distance\n",
        "    # cos_sim = linear_kernel(classes_embeddings, taxo_embeddings)\n",
        "    cos_sim = cosine_similarity(classes_embeddings_piece, taxo_embeddings)\n",
        "    \n",
        "    print(i)\n",
        "    top_idx = []\n",
        "\n",
        "    # The target embedding has the smallest distance\n",
        "    for cs in cos_sim:\n",
        "        top_idx.append(np.argmax(cs))\n",
        "        \n",
        "    #len(top_idx)\n",
        "    \n",
        "    # Create the label column in the df\n",
        "    df2['label'] = level_2[top_idx]\n",
        "    df2.drop(columns=['categories'], inplace=True)\n",
        "    \n",
        "    df3 = df3.append(df2, ignore_index = True)\n",
        "    \n",
        "#     #classes_embeddings = np.append(classes_embeddings, classes_embeddings_piece, axis=0)\n",
        "#     fn = 'arr_'+str(i)+'.pkl'\n",
        "#     np.save(fn, classes_embeddings_piece, allow_pickle=True)\n",
        "    \n",
        "#     df2 = None; classes_embeddings_piece = None\n",
        "    "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-d22a3b68ca70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# generate embeddings for the categories entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclasses_embeddings_piece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_use_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# We now have the embeddings for both the categories data (the source) and the taxonomy (the target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d124eb92ed94>\u001b[0m in \u001b[0;36mget_use_matrix\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mxtrain_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUvQ_NG7Xgrg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38716272-0f3b-4fba-ff68-857f56b727ae"
      },
      "source": [
        "df3.shape"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(112120, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU44C9wcVcBC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bce5fc69-5f25-41f8-a810-9a816a8d905a"
      },
      "source": [
        "df2.shape"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5606, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHOf8DhyfOjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_feather('df_mapped_label_may15')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}