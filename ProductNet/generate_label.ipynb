{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_label.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHsMBaABXuGS",
        "colab_type": "text"
      },
      "source": [
        "# ProductNet: Categorize products using image(s) and text descriptors\n",
        "## Jen Sheng Wong and Kartik Nanda (Cohort 14)\n",
        "Based on following paper: https://arxiv.org/pdf/1904.09037.pdf\n",
        "\n",
        "## Problem Statement:\n",
        "Relates primarily to products on retail/marketplace sites such as Amazon. The problem deals with 3 main aspects:\n",
        "* Trying to categorize products. Number of categories ~5000 (using the Google taxonomy: https://github.com/fellowship/platform-demos3/blob/master/ProductNet/taxonomy-with-ids.en-US.xls)\n",
        "* Product has images (1 or more)\n",
        "* Product has text - title, description, keywords\n",
        "\n",
        "Possible end-problems to solve: \n",
        "a) Find the category, given product images and user provided text description. \n",
        "b) Find mis-categorized products\n",
        "\n",
        "## Dataset: \n",
        "Products for sale on Amazon; product details from 1996 through 2014, by Prof. McAuley at UCSD\n",
        "Citations:\n",
        "* R. He, J. McAuley. Modeling the visual evolution of fashion trends with one-class collaborative filtering. WWW, 2016\n",
        "* J. McAuley, C. Targett, J. Shi, A. van den Hengel. Image-based recommendations on styles and substitutes. SIGIR, 2015\n",
        "\n",
        "## Dataset Storage:\n",
        "* Used Google storage bucket located at gs://platform-ai-research/datasets/ProductNet/\n",
        "* Also used Jen Sheng's Google Drive for intermediate files, images etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj2Rh8RLXuGU",
        "colab_type": "text"
      },
      "source": [
        "## Generating labels for the dataset\n",
        "The dataset has three text fields - categories, description and title. Any and all of these can be used to generate labels for the dataset. \n",
        "\n",
        "The first attempt involved using the 'categories' entry in the dataset itself as the label. This results in ~90,000 unique labels. The error_rates working with this were in the range of 98%.\n",
        "\n",
        "The second attempt extracted the label from the categories by picking the first category entry in the categories column. This reduced the number of unique labels to ~40 for a smaller sampled dataset (10k instead of 5.7 million). error-rate was ~70%\n",
        "\n",
        "Third attempt generates labels by mapping the categories data to google taxonomy entries. We then use these new labels to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LHsRqO0XuGV",
        "colab_type": "code",
        "outputId": "73bfbe6e-d79a-4f04-b5f8-f9fde90138c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        }
      },
      "source": [
        "!pip install fastai\n",
        "!pip install pyarrow\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from fastai.vision import *\n",
        "from fastai.metrics import error_rate, accuracy\n",
        "\n",
        "warnings.simplefilter(action='ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastai in /usr/local/lib/python3.6/dist-packages (1.0.52)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.1.21)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.352.0)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai) (1.2.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.1.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from fastai) (3.6.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.2.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (4.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.24.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.6.9)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.21.0)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.0.18)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.2.2.post3)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.0.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (19.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai) (0.46)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (6.12.1)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.35)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.9.6)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2018.1.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.1)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->fastai) (1.12.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.1.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (4.28.1)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.5.6)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (1.10.11)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.4.3.2)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.9.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->fastai) (41.0.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.9.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (0.13.0)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.16.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVnaza8GXuGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the cleaned dataset from the GS bucket\n",
        "# Set the file name - this is the output file from step1 (workbook_1)\n",
        "file_name = 'metadata_clean_0513'\n",
        "gs_path = 'platform-ai-research/datasets/ProductNet/'     # location of the bucket\n",
        "\n",
        "# set local to False if running on Colab\n",
        "local = False      # this needs to be automated (how?)\n",
        "\n",
        "# Reading in the datafile\n",
        "exists = os.path.isfile(file_name)\n",
        "if (not exists):\n",
        "    if (not local):\n",
        "        # Login to access the GS bucket\n",
        "        from google.colab import auth\n",
        "        auth.authenticate_user()\n",
        "\n",
        "        # Copy the datafile to the Colab local dir\n",
        "        try:\n",
        "            remote_file = gs_path + file_name\n",
        "            !gsutil cp gs://{remote_file} {file_name}\n",
        "        except Exception as e:\n",
        "            print('File Does Not Exist')\n",
        "            sys.exit()\n",
        "                \n",
        "    else: \n",
        "        print('File Does Not Exist')\n",
        "        sys.exit()\n",
        "\n",
        "# Read in the file\n",
        "df = pd.read_feather(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1aiON4dXuGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "00461f32-cb8f-418a-91f5-9e70c9930236"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5607160 entries, 0 to 5607159\n",
            "Data columns (total 7 columns):\n",
            "index          int64\n",
            "asin           object\n",
            "categories     object\n",
            "description    object\n",
            "imUrl          object\n",
            "title          object\n",
            "label          object\n",
            "dtypes: int64(1), object(6)\n",
            "memory usage: 299.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l49Ifgf7XuGh",
        "colab_type": "code",
        "outputId": "d10a2c65-24e7-4776-91c6-4868d05ea863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "df = df[['asin', 'categories']]    # keep only the asin and categories columns\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asin</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000037214</td>\n",
              "      <td>Clothing, Shoes &amp; Jewelry, Girls, Clothing, Sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000032069</td>\n",
              "      <td>Sports &amp; Outdoors, Other Sports, Dance, Clothi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000031909</td>\n",
              "      <td>Sports &amp; Outdoors, Other Sports, Dance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0000032034</td>\n",
              "      <td>Sports &amp; Outdoors, Other Sports, Dance, Clothi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000031852</td>\n",
              "      <td>Sports &amp; Outdoors, Other Sports, Dance</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         asin                                         categories\n",
              "0  0000037214  Clothing, Shoes & Jewelry, Girls, Clothing, Sh...\n",
              "1  0000032069  Sports & Outdoors, Other Sports, Dance, Clothi...\n",
              "2  0000031909             Sports & Outdoors, Other Sports, Dance\n",
              "3  0000032034  Sports & Outdoors, Other Sports, Dance, Clothi...\n",
              "4  0000031852             Sports & Outdoors, Other Sports, Dance"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCWUsuDf7jY7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f84b45d-c6fc-404a-d16e-ab9a4c5a25a4"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5607160"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEpugCU9YNzd",
        "colab_type": "code",
        "outputId": "0e95e5e4-19fe-42e0-978b-cee1caa8311d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Using the Universal Sentence Encoder to map categories to google taxonomy labels\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "hub_embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0515 20:37:55.286013 139745971382144 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiIt5XRJ7lWT",
        "colab_type": "text"
      },
      "source": [
        "# Generate embeddings for Google Taxonomy \n",
        "Here, we make use of level 1 and level 2 as labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ipMi5Jv7aj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "787e6805-06b8-4965-cb19-c332d88af7ac"
      },
      "source": [
        "# Now generate the embeddings for the taxonomy - the labels we are trying to map to\n",
        "# Read in the taxonomy file from the bucket\n",
        "file_name = 'taxonomy-with-ids.en-US.xls'\n",
        "\n",
        "remote_file = gs_path + file_name\n",
        "!gsutil cp gs://{remote_file} ."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://platform-ai-research/datasets/ProductNet/taxonomy-with-ids.en-US.xls...\n",
            "/ [0 files][    0.0 B/603.0 KiB]                                                \r/ [1 files][603.0 KiB/603.0 KiB]                                                \r\n",
            "Operation completed over 1 objects/603.0 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik1qZmDv7W7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the taxonomy file\n",
        "taxo = pd.read_excel('taxonomy-with-ids.en-US.xls')\n",
        "\n",
        "taxo = taxo.fillna('')\n",
        "\n",
        "# get columns. Different columns provide a different depth into the taxonomy.\n",
        "# For example column 1 would be top level, and has few (31) categories\n",
        "# Column 2 is one level deeper, and has 129 labels, column 3 is deeper still, and so on\n",
        "level_2 = taxo.iloc[:, 2].unique()\n",
        "level_1 = taxo.iloc[:, 1].unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O3hptkBNu-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "89f4ed66-71d0-4267-82e7-ddf92ed883b7"
      },
      "source": [
        "taxo=None; gc.collect()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeqEH7b6YmZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate the embeddings for various text input\n",
        "def get_use_matrix(s):\n",
        "    embeddings = hub_embed(s)\n",
        "\n",
        "    with tf.Session() as session:\n",
        "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "        xtrain_embeddings = session.run(embeddings)\n",
        "        \n",
        "    return xtrain_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he11URft6sqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "00e4a4f7-f548-44a5-d6b4-746f685ba592"
      },
      "source": [
        "taxo_embeddings_l1 = get_use_matrix(level_1)\n",
        "taxo_embeddings_l2 = get_use_matrix(level_2)\n",
        "\n",
        "taxo_embeddings_l1.shape, taxo_embeddings_l2.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21, 512), (192, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdhXoB2E7g4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We now have the embeddings for both the categories data (the source) and the taxonomy (the target)\n",
        "# Need to map the source (categories embeddings) to the target.\n",
        "# the label is the embedding in the taxonomy that is \"closest\" to the categories embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFGcy608CG30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next step is to download the images\n",
        "if (not os.path.exists('./labels/')):\n",
        "    !mkdir 'labels'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDfBoWKLEMm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Can't run the entire dataset on colab (not enough RAM) so trying in pieces\n",
        "pieces = 400; n = int(df.shape[0]/pieces)\n",
        "classes_embeddings = np.empty(shape=[0, 512])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1bpsujRrnvm",
        "colab_type": "code",
        "outputId": "eb3c461d-e6d4-4066-c75d-6cade1852574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1988
        }
      },
      "source": [
        "START = 5 # can change to i if i is not None\n",
        "END = pieces\n",
        "\n",
        "for i in range(START, END):\n",
        "    print(i)\n",
        "    df2 = df.loc[i*n:(i+1)*n-1]\n",
        "    \n",
        "    # generate embeddings for the categories entries\n",
        "    s = df2.categories.values\n",
        "\n",
        "    # get embeddings for the all entries in a piece\n",
        "    classes_embeddings_piece = get_use_matrix(s)\n",
        "    \n",
        "    # Calculate the distance\n",
        "    cos_sim_l1 = cosine_similarity(classes_embeddings_piece, taxo_embeddings_l1)\n",
        "    cos_sim_l2 = cosine_similarity(classes_embeddings_piece, taxo_embeddings_l2)\n",
        "    \n",
        "    # Get the closest label\n",
        "    top_1_label = [level_1[np.argmax(cs)] for cs in cos_sim_l1]\n",
        "    \n",
        "    # Closest top N labels for multiclass\n",
        "    N = 3 # Try top 3\n",
        "    \n",
        "    # Classes probability \n",
        "    class_prob = [np.argsort(cs)[::-1][:N] for cs in cos_sim_l2]\n",
        "    top_3_labels = [level_2[idx] for idx in class_prob]    \n",
        "    \n",
        "    # Assign to df    \n",
        "    df2['top_1'] = top_1_label\n",
        "    df2['top_3'] = top_3_labels\n",
        "    df2['top_3'] = df2['top_3'].apply(lambda x: ', '.join(map(str, x)))\n",
        "    \n",
        "    df_idx = len(df2)*(i+1)\n",
        "    \n",
        "    fn = './labels/labels_' + str(df_idx)\n",
        "    \n",
        "    print(fn)\n",
        "    \n",
        "    df2 = df2.reset_index(drop=True)\n",
        "    df2.to_feather(fn)\n",
        "       \n",
        "    df2 = None; \n",
        "    classes_embeddings_piece = None;\n",
        "    cos_sim_l1 = None; cos_sim_l2 = None; \n",
        "    top_1_label = None; top_3_labels = None;\n",
        "    \n",
        "    gc.collect()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[7288840,65] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node module_apply_default_2/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node module_apply_default_2/Encoder_en/hidden_layers/l2_normalize}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1d312ca22fb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# get embeddings for the all entries in a piece\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mclasses_embeddings_piece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_use_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Calculate the distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d124eb92ed94>\u001b[0m in \u001b[0;36mget_use_matrix\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mxtrain_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mxtrain_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[7288840,65] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node module_apply_default_2/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/Softmax (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py:547) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node module_apply_default_2/Encoder_en/hidden_layers/l2_normalize (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py:547) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'module_apply_default_2/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/Softmax', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-1d312ca22fb2>\", line 12, in <module>\n    classes_embeddings_piece = get_use_matrix(s)\n  File \"<ipython-input-10-d124eb92ed94>\", line 2, in get_use_matrix\n    embeddings = hub_embed(s)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module.py\", line 250, in __call__\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\", line 547, in create_apply_graph\n    import_scope=relative_scope_name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1435, in import_meta_graph\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1457, in _import_meta_graph_with_return_elements\n    **kwargs))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\n    return_elements=return_elements)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py\", line 235, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3433, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3433, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3325, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[7288840,65] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node module_apply_default_2/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/Softmax (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py:547) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node module_apply_default_2/Encoder_en/hidden_layers/l2_normalize (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py:547) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAgN0CDOOMVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "read = pd.read_feather(fn)\n",
        "\n",
        "read.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw1K-Ib2N89Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "751259f3-fd3b-4c90-89a2-c7d7b3ec4545"
      },
      "source": [
        "# PUSH TO GS\n",
        "gs_path = 'platform-ai-research/datasets/ProductNet/'\n",
        "file_name = 'labels'\n",
        "\n",
        "remote_file = gs_path + file_name\n",
        "!gsutil cp -r {file_name} gs://{remote_file} "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://labels/labels_28035 [Content-Type=application/octet-stream]...\n",
            "Copying file://labels/labels_56068 [Content-Type=application/octet-stream]...\n",
            "Copying file://labels/labels_11214 [Content-Type=application/octet-stream]...\n",
            "Copying file://labels/labels_70085 [Content-Type=application/octet-stream]...\n",
            "\\ [4 files][ 11.5 MiB/ 11.5 MiB]                                                \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying file://labels/labels_14017 [Content-Type=application/octet-stream]...\n",
            "Copying file://labels/labels_42051 [Content-Type=application/octet-stream]...\n",
            "| [6 files][ 16.1 MiB/ 16.1 MiB]                                                \n",
            "Operation completed over 6 objects/16.1 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh6RNarubnpq",
        "colab_type": "code",
        "outputId": "673952f6-323d-4a12-dcee-86d2ef9f8c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Embeddings for the categories have been generated\n",
        "classes_embeddings.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(173817, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPBneGAnb1sk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3e29805-9c77-4f7e-b076-d8ee2d48e340"
      },
      "source": [
        "level_2[:5]\n",
        "\n",
        "# np.argsort(cs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Live Animals', 'Pet Supplies', '', 'Clothing', 'Clothing Accessories'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV5uosaKdtyh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ac45d3f0-0426-4b0a-a70b-90e5d430c448"
      },
      "source": [
        "idxs = [[4, 3, 2, 1], [1, 2, 3, 4]]\n",
        "\n",
        "[level_2[idx] for idx in np.argsort(idxs)[::-1][:3]]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['Live Animals', 'Pet Supplies', '', 'Clothing'], dtype=object),\n",
              " array(['Clothing', '', 'Pet Supplies', 'Live Animals'], dtype=object)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9nSV7hse2YK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52c6f41d-acfe-40d1-c6e3-79593cefa776"
      },
      "source": [
        "np.argmax(idxs)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYOfVS6cfJGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the label column in the df\n",
        "df['label'] = level_2[top_idx]\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHOf8DhyfOjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_feather('df_mapped_label_may15')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}