{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding Dictionary\n",
    "\n",
    "Before defining & training our model, we need suitable embeddings. We base all our work on Google's word2vec. It's a massive embedding dictionary containing the most common 300k words (based on Google news), with a 300 dimensional embedding for each word. Given this vocabulary, we will find almost any food item. \n",
    "\n",
    "You can find the pre-trained embeddings [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit).\n",
    "\n",
    "However, we need a little bit of pre-processing.\n",
    "\n",
    "1. We need a function that maps labels to an appropriate list of words. Each element in this list should appear in our embedding dictionary. word2vec usually contains both lower and upper case versions of a word. We generally want to use the lower case version (in the case of food 'apple' instead of 'Apple' as the later can also refer to the company), except for proper nouns (for example 'Babybel').\n",
    "2. We don't need most of the 300k words in word2vec, and using all words takes up too much memory (around 4-5 GB). Instead, we want to create a subset based on the food items we are interested in.\n",
    "3. We create the embedding dictionaries for both our full set of labels and the reduced set of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.vision import *\n",
    "import gensim\n",
    "\n",
    "from ImageEmbedModel import utils\n",
    "from tqdm import tqdm\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/jupyter/data')\n",
    "\n",
    "labels = pd.read_csv(path/'food_label_concat_new.csv')\n",
    "labels_red = pd.read_csv(path/'food_label_concat_new_red.csv')\n",
    "labels['label'] = labels['label'].apply(str)\n",
    "labels_red['label'] = labels_red['label'].apply(str)\n",
    "\n",
    "extra_labels = ['salty', 'sweet', 'sour', 'hot', 'bitter', 'large', 'small', 'liquid', 'roasted', 'sauteed', 'boiled',\n",
    "                'pepper', 'salt', 'fruity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec (takes a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(path/'GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get classes for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total classes: 304\n",
      "\n",
      "total classes reduced: 201\n"
     ]
    }
   ],
   "source": [
    "all_classes = utils.get_labels(labels['label'])\n",
    "all_classes_red = utils.get_labels(labels_red['label'])\n",
    "all_classes = all_classes+extra_labels\n",
    "all_classes_red = all_classes_red+extra_labels\n",
    "print(\"total classes: {}\\n\".format(len(all_classes)))\n",
    "print(\"total classes reduced: {}\".format(len(all_classes_red)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create label dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_dict(all_classes) -> dict:\n",
    "    res = {}\n",
    "    errors = []\n",
    "    for label in all_classes:\n",
    "        for word in utils.get_lab_list(label):\n",
    "            try: \n",
    "                res[word] = model.get_vector(word)\n",
    "            except:\n",
    "                errors.append(label)\n",
    "                print(\"error: {}\".format(label))\n",
    "    return res, errors        \n",
    "\n",
    "def dump_dict(class_dict,path):\n",
    "    pickle.dump(class_dict, open(path, 'wb'))\n",
    "\n",
    "def remove_errors(df,errors):\n",
    "    df2 = df[df['label'].apply(lambda x: x not in errors)]\n",
    "    removed_count = len(df) - len(df2)\n",
    "    return df2, removed_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: slad\n"
     ]
    }
   ],
   "source": [
    "total_dict, err_total = create_label_dict(all_classes)\n",
    "total_dict_red, err_total_red = create_label_dict(all_classes_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fn_red = path/'food_dict_red.pkl'\n",
    "out_fn_new = path/'food_dict_new.pkl'\n",
    "dump_dict(total_dict,out_fn_new)\n",
    "dump_dict(total_dict_red,out_fn_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset labels so that they don't contain errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels removed: 0\n",
      "\n",
      "labels reduced removed: 1534\n"
     ]
    }
   ],
   "source": [
    "labels_clean, removed_count = remove_errors(labels,err_total)\n",
    "labels_red_clean, removed_count_red = remove_errors(labels_red,err_total_red)\n",
    "print(\"labels removed: {}\\n\".format(removed_count))\n",
    "print(\"labels reduced removed: {}\".format(removed_count_red))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_clean.to_csv( path/'all_food_processed_new.csv', index=False)\n",
    "labels_red_clean.to_csv( path/'all_food_processed_red.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
